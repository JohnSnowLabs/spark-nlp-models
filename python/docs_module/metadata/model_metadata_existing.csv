title,labels,author,name,latest_date,tags,description,dataset_info,included_models,dimension,edition,language,license,compatibility,inputs,case_sensitive,output,type,download_url,demo_url,colab_url,file,spark_version,ts,dataset
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2020-02-03,"[ner, fr, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org),,,Official,fr,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_fr_2.4.0_2.4_1579717534654.zip,https://demo.johnsnowlabs.com/public/NER_FR,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_FR.ipynb,wikiner_6B_300_fr_2.4.0_2.4_1579717534654.zip,2.4,1579717534654,mds
Deidentification NER (Enriched),,John Snow Labs,ner_deid_enriched,2020-03-04,"[ner, en, deidentify, licensed]","Deidentification NER (Enriched) is a Named Entity Recognition model that annotates text to find protected health information that may need to be deidentified. The entities it annotates are Age, City, Country, Date, Doctor, Hospital, Idnum, Medicalrecord, Organization, Patient, Phone, Profession, State, Street, Username, and Zip. Clinical NER is trained with the 'embeddings_clinical' word embeddings model, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/](https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/),,,Official,en,Licensed,2.4.2,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_deid_enriched_en_2.4.2_2.4_1587513306751.zip,https://demo.johnsnowlabs.com/healthcare/NER_DEMOGRAPHICS,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/NER_DEMOGRAPHICS.ipynb,ner_deid_enriched_en_2.4.2_2.4_1587513306751.zip,2.4,1587513306751,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2019-07-13,"[open_source, ner, fr]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org),,,Official,fr,Open Source,2.1.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_fr_2.1.0_2.4_1563035043013.zip,https://demo.johnsnowlabs.com/public/NER_FR,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_FR.ipynb,wikiner_840B_300_fr_2.1.0_2.4_1563035043013.zip,2.4,1563035043013,mds
Ner DL Model,"Age, Diagnosis, Dosage, Drug_name, Frequency, Gender, Lab_name, Lab_result, Symptom_name",John Snow Labs,ner_jsl,2020-04-22,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for clinical terminology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on data gathered and manually annotated by John Snow Labs.
https://www.johnsnowlabs.com/data/",,,Healthcare,en,Licensed,2.4.2,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_jsl_en_2.4.2_2.4_1587513304751.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_jsl_en_2.4.2_2.4_1587513304751.zip,2.4,1587513304751,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2019-07-13,"[open_source, ner, de]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://de.wikipedia.org](https://de.wikipedia.org),,,Official,de,Open Source,2.1.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_de_2.1.0_2.4_1563035544700.zip,https://demo.johnsnowlabs.com/public/NER_DE,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_DE.ipynb,wikiner_840B_300_de_2.1.0_2.4_1563035544700.zip,2.4,1563035544700,mds
Clinical NER (Large),,John Snow Labs,ner_clinical_large,2020-05-10,"[ner, en, licensed]","Clinical NER (Large) is a Named Entity Recognition model that annotates text to find references to clinical events. The entities it annotates are Problem, Treatment, and Test. Clinical NER is trained with the 'embeddings_clinical' word embeddings model, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/](https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/),,,Official,en,Licensed,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_large_clinical_en_2.5.0_2.4_1590021302624.zip,https://demo.johnsnowlabs.com/healthcare/NER_EVENTS_CLINICAL,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/NER_EVENTS_CLINICAL.ipynb,ner_large_clinical_en_2.5.0_2.4_1590021302624.zip,2.4,1590021302624,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2020-03-16,"[ner, ru, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://ru.wikipedia.org](https://ru.wikipedia.org),,,Official,ru,Open Source,2.4.4,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_ru_2.4.4_2.4_1584014001695.zip,https://demo.johnsnowlabs.com/public/NER_RU,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_RU.ipynb,wikiner_840B_300_ru_2.4.4_2.4_1584014001695.zip,2.4,1584014001695,mds
NerDLModel Bionlp,"Amino_acid, Anatomical_system, Cancer, Cell, Cellular_component, Developing_anatomical_Structure, Gene_or_gene_product, Immaterial_anatomical_entity, Multi,tissue_structure, Organ, Organism, Organism_subdivision, Simple_chemical, Tissue",John Snow Labs,ner_bionlp,2020-01-30,"[licensed, ner, en]","Pretrained named entity recognition deep learning model for biology and genetics terms. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on Cancer Genetics (CG) task of the BioNLP Shared Task 2013 with 'embeddings_clinical'.
http://2013.bionlp-st.org/tasks/cancer-genetics",,,Healthcare,en,Licensed,2.4.0,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_bionlp_en_2.4.0_2.4_1580237286004.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_bionlp_en_2.4.0_2.4_1580237286004.zip,2.4,1580237286004,mds
ALBERT Base Uncase,,John Snow Labs,albert_base_uncased,2020-04-28,"[embeddings, en, open_source]","ALBERT is ""A Lite"" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter,reduction techniques that allow for large,scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper ""[ALBERT: A Lite BERT for Self,supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)""",The model is imported from [https://tfhub.dev/google/albert_base/3](https://tfhub.dev/google/albert_base/3),,768,Official,en,Open Source,2.5.0,"sentence, token",false,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_base_uncased_en_2.5.0_2.4_1588073363475.zip,,,albert_base_uncased_en_2.5.0_2.4_1588073363475.zip,2.4,1588073363475,mds
WikiNER 6B 100,,John Snow Labs,wikiner_6B_100,2020-05-10,"[ner, pt, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://pt.wikipedia.org](https://pt.wikipedia.org),,,Official,pt,Open Source,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_pt_2.5.0_2.4_1588495233192.zip,https://demo.johnsnowlabs.com/public/NER_PT,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PT.ipynb,wikiner_6B_100_pt_2.5.0_2.4_1588495233192.zip,2.4,1588495233192,mds
NerDLModel Posology Small,"Dosage, Drug, Duration, Form, Frequency, Route, Strength",John Snow Labs,ner_posology_small,2020-04-22,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for posology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on the 2018 i2b2 dataset with 'embeddings_clinical'.
https://www.i2b2.org/NLP/Medication",,,Healthcare,en,Licensed,2.4.2,"sentence,token, embeddings",false,ner,ner,https://s3.amazon.com/auxdata.johnsnowlabs.com/clinical/models/ner_posology_small_en_2.4.2_2.4_1587513301751.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_posology_small_en_2.4.2_2.4_1587513301751.zip,2.4,1587513301751,mds
GloVe 6B 300,,John Snow Labs,glove_6B_300,2020-02-03,"[embeddings, en, open_source]","GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.",The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/),,300,Official,en,Open Source,2.4.0,sentence. token,false,embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.4.0_2.4_1579698630432.zip,https://demo.johnsnowlabs.com/public/NER_EN,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb,glove_6B_300_xx_2.4.0_2.4_1579698630432.zip,2.4,1579698630432,mds
Explain Document DL,,John Snow Labs,explain_document_dl,,"[pipeline, en, open_source]",The *explain_document_dl* is a pretrained pipeline that we can use to process text with a simple pipeline that performs basic processing steps.,,"The explain_document_ml has one Transformer and six annotators:
- Documenssembler - A Transformer that creates a column that contains documents.
- Sentence Segmenter - An annotator that produces the sentences of the document.
- Tokenizer - An annotator that produces the tokens of the sentences.
- SpellChecker - An annotator that produces the spelling-corrected tokens.
- Stemmer - An annotator that produces the stems of the tokens.
- Lemmatizer - An annotator that produces the lemmas of the tokens.
- POS Tagger - An annotator that produces the parts of speech of the associated tokens.",,Community,en,Open Source,2.5.5,,,,pipeline,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/explain_document_dl_en_2.4.3_2.4_1584626657780.zip,,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/explain-document-ml/explain_document_ml.ipynb,explain_document_dl_en_2.4.3_2.4_1584626657780.zip,2.4,1584626657780,mds
BERT Large Cased,,John Snow Labs,bert_large_cased,2020-01-02,"[open_source, embeddings, en]","This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper ""[BERT: Pre,training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)"".",The model is imported from [https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16/1](https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16/1),,1024,Official,en,Open Source,2.4.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_large_cased_en_2.4.0_2.4_1580580251298.zip,,,bert_large_cased_en_2.4.0_2.4_1580580251298.zip,2.4,1580580251298,mds
Universal Sentence Encoder,,John Snow Labs,tfhub_use,2020-04-17,"[embeddings, en, open_source]","The Universal Sentence Encoder encodes text into high,dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.,The model is trained and optimized for greater,than,word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. We apply this model to the STS benchmark for semantic similarity, and the results can be seen in the example notebook made available. The universal,sentence,encoder model is trained with a deep averaging network (DAN) encoder.,The details are described in the paper ""[Universal Sentence Encoder](https://arxiv.org/abs/1803.11175)"".",The model is imported from [https://tfhub.dev/google/universal-sentence-encoder/2](https://tfhub.dev/google/universal-sentence-encoder/2),,512,Official,en,Open Source,2.4.0,sentence,true,sentence_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/tfhub_use_en_2.4.0_2.4_1587136330099.zip,,,tfhub_use_en_2.4.0_2.4_1587136330099.zip,2.4,1587136330099,mds
BioBERT Clinical,,John Snow Labs,biobert_clinical_base_cased,2020-07-20,"[embeddings, en, open_source]","This model contains a pre,trained weights of ClinicalBERT for generic clinical text. This domain,specific model has performance improvements on 3/5 clinical NLP tasks andd establishing a new state,of,the,art on the MedNLI dataset. The details are described in the paper ""[Publicly Available Clinical BERT Embeddings](https://www.aclweb.org/anthology/W19,1909/)"".",The model is imported from [https://github.com/EmilyAlsentzer/clinicalBERT](https://github.com/EmilyAlsentzer/clinicalBERT),,768,Official,en,Open Source,2.5.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_clinical_base_cased_en_2.5.0_2.4_1590489819943.zip,,,biobert_clinical_base_cased_en_2.5.0_2.4_1590489819943.zip,2.4,1590489819943,mds
Explain Clinical Doc ERA,,John Snow Labs,explain_clinical_doc_era,2020-08-19,"[pipeline, en, licensed]","A pretrained pipeline with ner_clinical_events, assertion_dl and re_temporal_events_clinical. It will extract clinical entities, assign assertion status and find temporal relationships between clinical entities",,"- ner_clinical_events
- assertion_dl
- re_temporal_events_clinical",,Healthcare,en,Licensed,2.5.5,,,,pipeline,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/explain_clinical_doc_era_en_2.5.5_2.4_1597841630062.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/11.Pretrained_Clinical_Pipelines.ipynb,explain_clinical_doc_era_en_2.5.5_2.4_1597841630062.zip,2.4,1597841630062,mds
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2020-05-10,"[ner, pl, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://pl.wikipedia.org](https://pl.wikipedia.org),,,Official,pl,Open Source,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_pl_2.5.0_2.4_1588519719571.zip,https://demo.johnsnowlabs.com/public/NER_PL,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PL.ipynb,wikiner_6B_300_pl_2.5.0_2.4_1588519719571.zip,2.4,1588519719571,mds
BERT Base Uncased,,John Snow Labs,bert_base_uncased,2020-01-02,"[open_source, embeddings, en]","This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper ""[BERT: Pre,training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)"".",The model is imported from [https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1](https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1),,768,Official,en,Open Source,2.4.0,"sentence, token",false,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_en_2.4.0_2.4_1580579557778.zip,,,bert_base_cased_en_2.4.0_2.4_1580579557778.zip,2.4,1580579557778,mds
XLNet Large,,John Snow Labs,xlnet_large_cased,2020-04-28,"[embeddings, en, open_source]","XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer,XL as the backbone model, exhibiting excellent performance for language tasks involving long context. Overall, XLNet achieves state,of,the,art (SOTA) results on various downstream language tasks including question answering, natural language inference, sentiment analysis, and document ranking. The details are described in the paper ""[​XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)""",The model is imported from [https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet),,1024,Official,en,Open Source,2.5.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlnet_large_cased_en_2.5.0_2.4_1588074397954.zip,,,xlnet_large_cased_en_2.5.0_2.4_1588074397954.zip,2.4,1588074397954,mds
NerDLModel Clinical Large,"Problem, Test, Treatment",John Snow Labs,ner_clinical_large,2020-05-23,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for clinical terms. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on augmented 2010 i2b2 challenge data with 'embeddings_clinical'.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",,,Healthcare,en,Licenced,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_clinical_large_en_2.5.0_2.4_1590021302624.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_clinical_large_en_2.5.0_2.4_1590021302624.zip,2.4,1590021302624,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2020-05-10,"[ner, pl, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://pl.wikipedia.org](https://pl.wikipedia.org),,,Official,pl,Open Source,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_pl_2.5.0_2.4_1588519719572.zip,https://demo.johnsnowlabs.com/public/NER_PL,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PL.ipynb,wikiner_840B_300_pl_2.5.0_2.4_1588519719572.zip,2.4,1588519719572,mds
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2020-02-03,"[ner, es, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://es.wikipedia.org](https://es.wikipedia.org),,,Official,es,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_es_2.4.0_2.4_1581971942090.zip,https://demo.johnsnowlabs.com/public/NER_ES,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_ES.ipynb,wikiner_6B_300_es_2.4.0_2.4_1581971942090.zip,2.4,1581971942090,mds
BERT Large Uncased,,John Snow Labs,bert_large_uncased,2020-01-02,"[open_source, embeddings, en]","This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper ""[BERT: Pre,training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)"".",The model is imported from [https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1](https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1),,1024,Official,en,Open Source,2.4.0,"sentence, token",false,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_large_uncased_en_2.4.0_2.4_1580581306683.zip,,,bert_large_uncased_en_2.4.0_2.4_1580581306683.zip,2.4,1580581306683,mds
Deidentification NER (Large),,John Snow Labs,ner_deid_large,2020-03-04,"[ner, en, deidentify, licensed]","Deidentification NER (Large) is a Named Entity Recognition model that annotates text to find protected health information that may need to be deidentified. The entities it annotates are Age, Contact, Date, Id, Location, Name, and Profession. Clinical NER is trained with the 'embeddings_clinical' word embeddings model, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/](https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/),,,Official,en,Licensed,2.4.2,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_deid_large_en_2.4.2_2.4_1587513305751.zip,https://demo.johnsnowlabs.com/healthcare/NER_DEMOGRAPHICS,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/NER_DEMOGRAPHICS.ipynb,ner_deid_large_en_2.4.2_2.4_1587513305751.zip,2.4,1587513305751,mds
NerDLModel Drugs,DrugChem,John Snow Labs,ner_drugs,2020-03-25,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for Drugs. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on i2b2_med7 + FDA with 'embeddings_clinical'.
https://www.i2b2.org/NLP/Medication",,,Healthcare,en,Licensed,2.4.4,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_drugs_en_2.4.4_2.4_1584452534235.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_drugs_en_2.4.4_2.4_1584452534235.zip,2.4,1584452534235,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2020-02-03,"[ner, es, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://es.wikipedia.org](https://es.wikipedia.org),,,Official,es,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_es_2.4.0_2.4_1581971942091.zip,https://demo.johnsnowlabs.com/public/NER_ES,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_ES.ipynb,wikiner_840B_300_es_2.4.0_2.4_1581971942091.zip,2.4,1581971942091,mds
Glove 840B 300,,John Snow Labs,glove_840B_300,2020-01-22,"[open_source, embeddings]","GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.",The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/),,300,Official,xx,Open Source,2.4.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_840B_300_xx_2.4.0_2.4_1579698926752.zip,,,glove_840B_300_xx_2.4.0_2.4_1579698926752.zip,2.4,1579698926752,mds
Assertion DL Large,"hypothetical, present, absent, possible, conditional, associated_with_someone_else",John Snow Labs,assertion_dl_large,2020-05-21,"[ner, en, licensed]","Deep learning named entity recognition model for assertions. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text with 'embeddings_clinical'.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",,,Healthcare,en,Licensed,2.5.0,"sentence, ner_chunk, embeddings",false,assertion,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/assertion_dl_large_en_2.5.0_2.4_1590022282256.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.Clinical_Assertion_Model.ipynb,assertion_dl_large_en_2.5.0_2.4_1590022282256.zip,2.4,1590022282256,mds
XLNet Base,,John Snow Labs,xlnet_base_cased,2020-04-28,"[embeddings, en, open_source]","XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer,XL as the backbone model, exhibiting excellent performance for language tasks involving long context. Overall, XLNet achieves state,of,the,art (SOTA) results on various downstream language tasks including question answering, natural language inference, sentiment analysis, and document ranking. The details are described in the paper ""[​XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)""",The model is imported from [https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet),,768,Official,en,Open Source,2.5.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlnet_base_cased_en_2.5.0_2.4_1588074114942.zip,,,xlnet_base_cased_en_2.5.0_2.4_1588074114942.zip,2.4,1588074114942,mds
ALBERT XLarge Uncase,,John Snow Labs,albert_xlarge_uncased,2020-04-28,"[embeddings, en, open_source]","ALBERT is ""A Lite"" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter,reduction techniques that allow for large,scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper ""[ALBERT: A Lite BERT for Self,supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)""",The model is imported from [https://tfhub.dev/google/albert_xlarge/3](https://tfhub.dev/google/albert_xlarge/3),,2048,Official,en,Open Source,2.5.0,"sentence, token",false,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_xlarge_uncased_en_2.5.0_2.4_1588073443653.zip,,,albert_xlarge_uncased_en_2.5.0_2.4_1588073443653.zip,2.4,1588073443653,mds
ALBERT XXLarge Uncase,,John Snow Labs,albert_xxlarge_uncased,2020-04-28,"[embeddings, en, open_source]","ALBERT is ""A Lite"" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter,reduction techniques that allow for large,scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper ""[ALBERT: A Lite BERT for Self,supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)""",The model is imported from [https://tfhub.dev/google/albert_xlarge/3](https://tfhub.dev/google/albert_xlarge/3),,1024,Official,en,Open Source,2.5.0,"sentence, token",false,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_xxlarge_uncased_en_2.5.0_2.4_1588073588232.zip,,,albert_xxlarge_uncased_en_2.5.0_2.4_1588073588232.zip,2.4,1588073588232,mds
Wiki NER 6B 100,,John Snow Labs,wikiner_6B_100,2019-07-13,"[open_source, ner, de]","Wiki NER is a Named Entity Recognition (or NER) model, that can be used to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. Wiki NER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is trained based on data from [https://de.wikipedia.org](https://de.wikipedia.org),,,Official,de,Open Source,2.1.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_de_2.1.0_2.4_1564861417829.zip,https://demo.johnsnowlabs.com/public/NER_DE,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_DE.ipynb,wikiner_6B_300_de_2.1.0_2.4_1564861417829.zip,2.4,1564861417829,mds
BERT Base Cased,,John Snow Labs,bert_base_cased,2020-01-02,"[open_source, embeddings, en]","This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper ""[BERT: Pre,training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)"".",The model is imported from [https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16/1](https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16/1),,768,Official,en,Open Source,2.4.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_en_2.4.0_2.4_1580579557778.zip,,,bert_base_cased_en_2.4.0_2.4_1580579557778.zip,2.4,1580579557778,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2020-02-03,"[ner, it, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://it.wikipedia.org](https://it.wikipedia.org),,,Official,it,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_it_2.4.0_2.4_1579699913554.zip,https://demo.johnsnowlabs.com/public/NER_IT,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_IT.ipynb,wikiner_840B_300_it_2.4.0_2.4_1579699913554.zip,2.4,1579699913554,mds
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2020-03-16,"[ner, ru, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://ru.wikipedia.org](https://ru.wikipedia.org),,,Official,ru,Open Source,2.4.4,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_ru_2.4.4_2.4_1584014001694.zip,https://demo.johnsnowlabs.com/public/NER_RU,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_RU.ipynb,wikiner_6B_300_ru_2.4.4_2.4_1584014001694.zip,2.4,1584014001694,mds
ALBERT Large Uncase,,John Snow Labs,albert_large_uncased,2020-04-28,"[embeddings, en, open_source]","ALBERT is ""A Lite"" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter,reduction techniques that allow for large,scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper ""[ALBERT: A Lite BERT for Self,supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)""",The model is imported from [https://tfhub.dev/google/albert_large/3](https://tfhub.dev/google/albert_large/3),,1024,Official,en,Open Source,2.5.0,"sentence, token",false,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_large_uncased_en_2.5.0_2.4_1588073397355.zip,,,albert_large_uncased_en_2.5.0_2.4_1588073397355.zip,2.4,1588073397355,mds
Assertion DL,"Hypothetical, Present, Absent, Possible, Conditional, Associated_with_someone_else",John Snow Labs,assertion_dl,2020-01-30,"[licensed, ner, en]","Deep learning named entity recognition model for assertions. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text with 'embeddings_clinical'.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",,,Healthcare,en,Licensed,2.4.0,"sentence, ner_chunk, embeddings",false,assertion,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/assertion_dl_en_2.4.0_2.4_1580237286004.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.Clinical_Assertion_Model.ipynb,assertion_dl_en_2.4.0_2.4_1580237286004.zip,2.4,1580237286004,mds
NerDLModel Healthcare,"Problem, Test, Treatment",John Snow Labs,ner_healthcare,2020-03-26,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for healthcare. Includes Problem, Test and Treatment entities. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on 2010 i2b2 challenge data with 'embeddings_clinical'.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",,,Healthcare,en,Licensed,2.4.4,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_healthcare_en_2.4.4_2.4_1585188313964.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_healthcare_en_2.4.4_2.4_1585188313964.zip,2.4,1585188313964,mds
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2019-07-13,"[open_source, ner, fr]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org),,,Official,fr,Open Source,2.1.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_fr_2.1.0_2.4_1564817386216.zip,https://demo.johnsnowlabs.com/public/NER_FR,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_FR.ipynb,wikiner_6B_300_fr_2.1.0_2.4_1564817386216.zip,2.4,1564817386216,mds
NerDLModel Risk Factors,"CAD, Diabetes, Family_hist, Hyperlipidemia, Hypertension, Medication, Obese, PHI, Smoker",John Snow Labs,ner_risk_factors,2020-04-22,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for Heart Disease Risk Factors and Personal Health Information. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on plain n2c2 2014: De-identification and Heart Disease Risk Factors Challenge datasets with 'embeddings_clinical'.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/",,,Healthcare,en,Licensed,2.4.2,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_risk_factors_en_2.4.2_2.4_1587513300751.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_risk_factors_en_2.4.2_2.4_1587513300751.zip,2.4,1587513300751,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2020-02-03,"[ner, de, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://de.wikipedia.org](https://de.wikipedia.org),,,Official,de,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_de_2.4.0_2.4_1579699913555.zip,https://demo.johnsnowlabs.com/public/NER_DE,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_DE.ipynb,wikiner_840B_300_de_2.4.0_2.4_1579699913555.zip,2.4,1579699913555,mds
Deidentify (Large),,John Snow Labs,deidentify_large,2020-08-04,"[deid, en, licensed]","Deidentify (Large) is a deidentification model. It identifies instances of protected health information in text documents, and it can either obfuscate them (e.g., replacing names with different, fake names) or mask them (e.g., replacing ""2020,06,04"" with ""<DATE>""). This model is useful for maintaining HIPAA compliance when dealing with text documents that contain protected health information.",The model is imported from [https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/](https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/),,,Official,en,Licensed,2.5.5,"sentence, token, ner_chunk",false,obfuscated,deid,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/nerdl_deid_en_1.8.0_2.4_1545462443516.zip,https://demo.johnsnowlabs.com/healthcare/DEID_PHI_TEXT,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/DEID_PHI_TEXT.ipynb,nerdl_deid_en_1.8.0_2.4_1545462443516.zip,2.4,1545462443516,mds
Assertion ML,"Hypothetical, Present, Absent, Possible, Conditional, Associated_with_someone_else",John Snow Labs,assertion_ml,2020-01-30,"[licensed, ner, en]",Logistic regression based named entity recognition model for assertions.,"Trained on 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text with 'embeddings_clinical'.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",,,Healthcare,en,Licensed,2.4.0,"sentence, ner_chunk, embeddings",false,assertion,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/assertion_dl_large_en_2.5.0_2.4_1590022282256.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.Clinical_Assertion_Model.ipynb,assertion_dl_large_en_2.5.0_2.4_1590022282256.zip,2.4,1590022282256,mds
GloVe 840B 300,,John Snow Labs,glove_840B_300,2020-02-03,"[embeddings, en, open_source]","GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.",The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/),,300,Official,en,Open Source,2.4.0,sentence. token,false,embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_840B_300_xx_2.4.0_2.4_1579698926752.zip,https://demo.johnsnowlabs.com/public/NER_EN,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb,glove_840B_300_xx_2.4.0_2.4_1579698926752.zip,2.4,1579698926752,mds
WikiNER 6B 100,,John Snow Labs,wikiner_6B_100,2020-05-10,"[ner, nl, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org),,,Official,nl,Open Source,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_nl_2.5.0_2.4_1588546201140.zip,https://demo.johnsnowlabs.com/public/NER_NL,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_NL.ipynb,wikiner_6B_100_nl_2.5.0_2.4_1588546201140.zip,2.4,1588546201140,mds
Ner DL Model Enriched,"Age, Diagnosis, Dosage, Drug_name, Frequency, Gender, Lab_name, Lab_result, Symptom_name",John Snow Labs,ner_jsl_enriched,2020-04-22,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for clinical terminology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on data gathered and manually annotated by John Snow Labs.
https://www.johnsnowlabs.com/data/",,,Healthcare,en,Licensed,2.4.2,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_jsl_enriched_en_2.4.2_2.4_1587513303751.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_jsl_enriched_en_2.4.2_2.4_1587513303751.zip,2.4,1587513303751,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2019-07-13,"[open_source, ner, it]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://it.wikipedia.org](https://it.wikipedia.org),,,Official,it,Open Source,2.1.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_it_2.1.0_2.4_1563095099139.zip,https://demo.johnsnowlabs.com/public/NER_IT,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_IT.ipynb,wikiner_840B_300_it_2.1.0_2.4_1563095099139.zip,2.4,1563095099139,mds
Glove 6B 100,,John Snow Labs,glove_100d,2020-01-22,"[open_source, embeddings, en]","GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.",The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/),,100,Official,en,Open Source,2.4.0,"sentence, token",false,embeddings,word_embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.4.0_2.4_1579690104032.zip,https://demo.johnsnowlabs.com/public/NER_EN/,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_dl.ipynb,glove_100d_en_2.4.0_2.4_1579690104032.zip,2.4,1579690104032,mds
NerDLModel Clinical Events,"Date,Time,Problem,Test,Treatment,Occurence,Clinical_Dept,Evidential,Duration,Frequency,Admission,Discharge",John Snow Labs,ner_events_clinical,2020-03-25,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for clinical events. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on i2b2 events data with 'clinical_embeddings'.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",,,Healthcare,en,Licensed,2.5.0,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_events_clinical_en_2.5.0_2.4_1590021303624.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_events_clinical_en_2.5.0_2.4_1590021303624.zip,2.4,1590021303624,mds
Neoplasms NER,,John Snow Labs,ner_neoplasms,2020-07-03,"[ner, en, licensed]","Neoplasms NER is a Named Entity Recognition model that annotates text to find references to tumors. The only entity it annotates is MalignantNeoplasm. Neoplasms NER is trained with the 'embeddings_clinical' word embeddings model, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://temu.bsc.es/cantemist/](https://temu.bsc.es/cantemist/),,,Official,es,Licensed,2.5.3,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_neoplasms_es_2.5.3_2.4_1594168624415.zip,https://demo.johnsnowlabs.com/healthcare/NER_TUMOR,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/NER_TUMOR.ipynb,ner_neoplasms_es_2.5.3_2.4_1594168624415.zip,2.4,1594168624415,mds
BioBERT Discharge,,John Snow Labs,biobert_discharge_base_cased,2020-07-20,"[embeddings, en, open_source]","This model contains a pre,trained weights of ClinicalBERT for discharge summaries. This domain,specific model has performance improvements on 3/5 clinical NLP tasks andd establishing a new state,of,the,art on the MedNLI dataset. The details are described in the paper ""[Publicly Available Clinical BERT Embeddings](https://www.aclweb.org/anthology/W19,1909/)"".",The model is imported from [https://github.com/EmilyAlsentzer/clinicalBERT](https://github.com/EmilyAlsentzer/clinicalBERT),,768,Official,en,Open Source,2.5.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_discharge_base_cased_en_2.5.0_2.4_1590490193605.zip,,,biobert_discharge_base_cased_en_2.5.0_2.4_1590490193605.zip,2.4,1590490193605,mds
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2020-02-03,"[ner, de, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://de.wikipedia.org](https://de.wikipedia.org),,,Official,de,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_de_2.4.0_2.4_1579717534653.zip,https://demo.johnsnowlabs.com/public/NER_DE,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_DE.ipynb,wikiner_6B_300_de_2.4.0_2.4_1579717534653.zip,2.4,1579717534653,mds
BioBERT PMC,,John Snow Labs,biobert_pmc_base_cased,2020-07-20,"[embeddings, en, open_source]","This model contains a pre,trained weights of BioBERT, a language representation model for biomedical domain, especially designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. The details are described in the paper ""[BioBERT: a pre,trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)"".",The model is imported from [https://github.com/dmis-lab/biobert](https://github.com/dmis-lab/biobert),,768,Official,en,Open Source,2.5.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_pmc_base_cased_en_2.5.0_2.4_1590489029151.zip,,,biobert_pmc_base_cased_en_2.5.0_2.4_1590489029151.zip,2.4,1590489029151,mds
WikiNER 6B 100,,John Snow Labs,wikiner_6B_100,2020-03-16,"[ner, ru, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://ru.wikipedia.org](https://ru.wikipedia.org),,,Official,ru,Open Source,2.4.4,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_ru_2.4.4_2.4_1584014001452.zip,https://demo.johnsnowlabs.com/public/NER_RU,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_RU.ipynb,wikiner_6B_100_ru_2.4.4_2.4_1584014001452.zip,2.4,1584014001452,mds
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2020-05-10,"[ner, nl, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org),,,Official,nl,Open Source,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_nl_2.5.0_2.4_1588546201483.zip,https://demo.johnsnowlabs.com/public/NER_NL,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_NL.ipynb,wikiner_6B_300_nl_2.5.0_2.4_1588546201483.zip,2.4,1588546201483,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2020-02-03,"[ner, fr, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org),,,Official,fr,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_fr_2.4.0_2.4_1579699913554.zip,https://demo.johnsnowlabs.com/public/NER_FR,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_FR.ipynb,wikiner_840B_300_fr_2.4.0_2.4_1579699913554.zip,2.4,1579699913554,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2020-05-10,"[ner, nl, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org),,,Official,nl,Open Source,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_nl_2.5.0_2.4_1588546201484.zip,https://demo.johnsnowlabs.com/public/NER_NL,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_NL.ipynb,wikiner_840B_300_nl_2.5.0_2.4_1588546201484.zip,2.4,1588546201484,mds
WikiNER 840B 300,,John Snow Labs,wikiner_840B_300,2020-05-10,"[ner, pt, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://pt.wikipedia.org](https://pt.wikipedia.org),,,Official,pt,Open Source,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_pt_2.5.0_2.4_1588495233642.zip,https://demo.johnsnowlabs.com/public/NER_PT,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PT.ipynb,wikiner_840B_300_pt_2.5.0_2.4_1588495233642.zip,2.4,1588495233642,mds
Elmo,,John Snow Labs,elmo,2020-01-31,"[embeddings, en, open_source]","Computes contextualized word representations using character,based word representations and bidirectional LSTMs.,This model outputs fixed embeddings at each LSTM layer and a learnable aggregation of the 3 layers.,* `word_emb`: the character,based word representations with shape [batch_size, max_length, 512].  == word_emb,* `lstm_outputs1`: the first LSTM hidden state with shape [batch_size, max_length, 1024]. === lstm_outputs1,* `lstm_outputs2`: the second LSTM hidden state with shape [batch_size, max_length, 1024]. === lstm_outputs2,* `elmo`: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape [batch_size, max_length, 1024]  == elmo,The complex architecture achieves state of the art results on several benchmarks. Note that this is a very computationally expensive module compared to word embedding modules that only perform embedding lookups. The use of an accelerator is recommended.,The details are described in the paper ""[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)"".",The model is imported from [https://tfhub.dev/google/elmo/3](https://tfhub.dev/google/elmo/3),,5121024,Official,en,Open Source,2.4.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/elmo_en_2.4.0_2.4_1580488815299.zip,,,elmo_en_2.4.0_2.4_1580488815299.zip,2.4,1580488815299,mds
Onto 100,,John Snow Labs,onto_100,2020-02-03,"[ner, en, open_source]","Onto is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. Onto was trained on the OntoNotes text corpus. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. Onto 100 is trained with GloVe 100d word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19),,,Official,en,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/onto_100_en_2.4.0_2.4_1579729071672.zip,https://demo.johnsnowlabs.com/public/NER_EN_18,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb,onto_100_en_2.4.0_2.4_1579729071672.zip,2.4,1579729071672,mds
NerDLModel Posology Large,"Dosage, Drug, Duration, Form, Frequency, Route, Strength",John Snow Labs,ner_posology_large,2020-04-22,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for posology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on the 2018 i2b2 dataset and FDA Drug datasets with 'embeddings_clinical'.
https://open.fda.gov/",,,Healthcare,en,Licensed,2.4.2,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_posology_large_en_2.4.2_2.4_1587513302751.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_posology_large_en_2.4.2_2.4_1587513302751.zip,2.4,1587513302751,mds
NerDLModel Diseases,Disease,John Snow Labs,ner_diseases,2020-03-25,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for diseases. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on i2b2 with 'embeddings_clinical'.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",,,Healthcare,en,Licensed,2.4.4,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_diseases_en_2.4.4_2.4_1584452534235.zip,#,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_diseases_en_2.4.4_2.4_1584452534235.zip,2.4,1584452534235,mds
RelationExtractionModel Clinical,"TrIP: A certain treatment has improved or cured a medical problem (eg, ‘infection resolved with antibiotic course’),TrWP: A patient's medical problem has deteriorated or worsened because of or in spite of a treatment being administered (eg, ‘the tumor was growing despite the drain’),TrCP: A treatment caused a medical problem (eg, ‘penicillin causes a rash’),TrAP: A treatment administered for a medical problem (eg, ‘Dexamphetamine for narcolepsy’),TrNAP: The administration of a treatment was avoided because of a medical problem (eg, ‘Ralafen which is contra,indicated because of ulcers’),TeRP: A test has revealed some medical problem (eg, ‘an echocardiogram revealed a pericardial effusion’),TeCP: A test was performed to investigate a medical problem (eg, ‘chest x,ray done to rule out pneumonia’),PIP: Two problems are related to each other (eg, ‘Azotemia presumed secondary to sepsis’)",John Snow Labs,re_clinical,2020-08-09,"[re, en, licensed]",Models the set of clinical relations defined in the 2010 i2b2 relation challenge.,"Trained on augmented 2010 i2b2 challenge data with 'clinical_embeddings'.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",,,Healthcare,en,Licensed,2.5.5,"embeddings, pos_tags, ner_chunks, dependencies",false,relations,re,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/re_clinical_en_2.5.5_2.4_1596928426753.zip,,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/10.Clinical_Relation_Extraction.ipynb,re_clinical_en_2.5.5_2.4_1596928426753.zip,2.4,1596928426753,mds
Onto 300,,John Snow Labs,onto_300,2020-02-03,"[ner, en, open_source]","Onto is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. Onto was trained on the OntoNotes text corpus. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. Onto 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19),,,Official,en,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/onto_300_en_2.4.0_2.4_1579729071854.zip,https://demo.johnsnowlabs.com/public/NER_EN_18,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb,onto_300_en_2.4.0_2.4_1579729071854.zip,2.4,1579729071854,mds
WikiNER 6B 100,,John Snow Labs,wikiner_6B_100,2020-05-10,"[ner, pl, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://pl.wikipedia.org](https://pl.wikipedia.org),,,Official,pl,Open Source,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_pl_2.5.0_2.4_1588519719293.zip,https://demo.johnsnowlabs.com/public/NER_PL,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PL.ipynb,wikiner_6B_100_pl_2.5.0_2.4_1588519719293.zip,2.4,1588519719293,mds
NerDLModel Clinical,"Problem, Test, Treatment",John Snow Labs,ner_clinical,2020-01-30,"[licensed, ner, en]","Pretrained named entity recognition deep learning model for clinical terms. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on 2010 i2b2 challenge data with `embeddings_clinical`.
https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/",,,Healthcare,en,Licensed,2.4.0,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_clinical_en_2.4.0_2.4_1580237286004.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_clinical_en_2.4.0_2.4_1580237286004.zip,2.4,1580237286004,mds
BioBERT Pubmed,,John Snow Labs,biobert_pubmed_base_cased,2020-07-20,"[embeddings, en, open_source]","This model contains a pre,trained weights of BioBERT, a language representation model for biomedical domain, especially designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. The details are described in the paper ""[BioBERT: a pre,trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)"".",The model is imported from [https://github.com/dmis-lab/biobert](https://github.com/dmis-lab/biobert),,768,Official,en,Open Source,2.5.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_pubmed_base_cased_en_2.5.0_2.4_1590487367971.zip,,,biobert_pubmed_base_cased_en_2.5.0_2.4_1590487367971.zip,2.4,1590487367971,mds
NerDLModel Posology,"Dosage,Drug,Duration,Form,Frequency,Route,Strength",John Snow Labs,ner_posology,2020-04-15,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for posology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on the 2018 i2b2 dataset and FDA Drug datasets with 'embeddings_clinical'.
https://open.fda.gov/",,,Healthcare,en,Licensed,2.4.2,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_posology_en_2.4.4_2.4_1584452534235.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_posology_en_2.4.4_2.4_1584452534235.zip,2.4,1584452534235,mds
BioBERT Pubmed PMC,,John Snow Labs,biobert_pubmed_pmc_base_cased,2020-07-20,"[embeddings, en, open_source]","This model contains a pre,trained weights of BioBERT, a language representation model for biomedical domain, especially designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. The details are described in the paper ""[BioBERT: a pre,trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)"".",The model is imported from [https://github.com/dmis-lab/biobert](https://github.com/dmis-lab/biobert),,768,Official,en,Open Source,2.5.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_pubmed_pmc_base_cased_en_2.5.0_2.4_1590489367180.zip,,,biobert_pubmed_pmc_base_cased_en_2.5.0_2.4_1590489367180.zip,2.4,1590489367180,mds
Universal Sentence Encoder Large,,John Snow Labs,tfhub_use_lg,2020-04-17,"[embeddings, en, open_source]","The Universal Sentence Encoder encodes text into high,dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.,The model is trained and optimized for greater,than,word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. We apply this model to the STS benchmark for semantic similarity, and the results can be seen in the example notebook made available. The universal,sentence,encoder model is trained with a deep averaging network (DAN) encoder.,The details are described in the paper ""[Universal Sentence Encoder](https://arxiv.org/abs/1803.11175)"".",The model is imported from [https://tfhub.dev/google/universal-sentence-encoder-large/3](https://tfhub.dev/google/universal-sentence-encoder-large/3),,512,Official,en,Open Source,2.4.0,sentence,true,sentence_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/tfhub_use_lg_en_2.4.0_2.4_1587136993894.zip,,,tfhub_use_lg_en_2.4.0_2.4_1587136993894.zip,2.4,1587136993894,mds
Glove 6B 300,,John Snow Labs,glove_6B_300,2020-01-22,"[open_source, embeddings]","GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.",The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/),,300,Official,xx,Open Source,2.4.0,"sentence, token",false,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.4.0_2.4_1579698630432.zip,,,glove_6B_300_xx_2.4.0_2.4_1579698630432.zip,2.4,1579698630432,mds
NerDLModel Cellular,"DNA, Cell_type, Cell_line, RNA, Protein",John Snow Labs,ner_cellular,2020-04-22,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for molecular biology related terms. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on the JNLPBA corpus containing more than 2.404 publication abstracts with 'embeddings_clinical'.
http://www.geniaproject.org/",,,Healthcare,en,Licensed,2.4.2,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_cellular_en_2.4.2_2.4_1587513308751.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_cellular_en_2.4.2_2.4_1587513308751.zip,2.4,1587513308751,mds
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2020-02-03,"[ner, it, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://it.wikipedia.org](https://it.wikipedia.org),,,Official,it,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_it_2.4.0_2.4_1579717534334.zip,https://demo.johnsnowlabs.com/public/NER_IT,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_IT.ipynb,wikiner_6B_300_it_2.4.0_2.4_1579717534334.zip,2.4,1579717534334,mds
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2019-07-13,"[open_source, ner, it]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://it.wikipedia.org](https://it.wikipedia.org),,,Official,it,Open Source,2.1.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_it_2.1.0_2.4_1564906405608.zip,https://demo.johnsnowlabs.com/public/NER_IT,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_IT.ipynb,wikiner_6B_300_it_2.1.0_2.4_1564906405608.zip,2.4,1564906405608,mds
WikiNER 6B 300,,John Snow Labs,wikiner_6B_300,2020-05-10,"[ner, pt, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://pt.wikipedia.org](https://pt.wikipedia.org),,,Official,pt,Open Source,2.5.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_pt_2.5.0_2.4_1588495233641.zip,https://demo.johnsnowlabs.com/public/NER_PT,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PT.ipynb,wikiner_6B_300_pt_2.5.0_2.4_1588495233641.zip,2.4,1588495233641,mds
NerDLModel Anatomy,"Anatomical_system,Cell,Cellular_component,Developing_anatomical_structure,Immaterial_anatomical_entity,Multi,tissue_structure,Organ,Organism_subdivision,Organism_substance,Pathological_formation,Tissue",John Snow Labs,ner_anatomy,2020-04-22,"[ner, en, licensed]","Pretrained named entity recognition deep learning model for anatomy terms. Includes Anatomical_system, Cell, Cellular_component, Developing_anatomical_structure, Immaterial_anatomical_entity, Multi,tissue_structure, Organ, Organism_subdivision, Organism_substance, Pathological_formation, and Tissue entities. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM,CNN.","Trained on the Anatomical Entity Mention (AnEM) corpus with 'embeddings_clinical'.
http://www.nactem.ac.uk/anatomy/",,,Healthcare,en,Licensed,2.4.2,"sentence,token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_anatomy_en_2.4.2_2.4_1587513307751.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb,ner_anatomy_en_2.4.2_2.4_1587513307751.zip,2.4,1587513307751,mds
WikiNER 6B 100,,John Snow Labs,wikiner_6B_100,2020-02-03,"[ner, es, open_source]","WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.",The model is imported from [https://es.wikipedia.org](https://es.wikipedia.org),,,Official,es,Open Source,2.4.0,"sentence, token, embeddings",false,ner,ner,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_es_2.4.0_2.4_1581971941700.zip,https://demo.johnsnowlabs.com/public/NER_ES,https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_ES.ipynb,wikiner_6B_100_es_2.4.0_2.4_1581971941700.zip,2.4,1581971941700,mds
BioBERT Pubmed Large,,John Snow Labs,biobert_pubmed_large_cased,2020-07-20,"[embeddings, en, open_source]","This model contains a pre,trained weights of BioBERT, a language representation model for biomedical domain, especially designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. The details are described in the paper ""[BioBERT: a pre,trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)"".",The model is imported from [https://github.com/dmis-lab/biobert](https://github.com/dmis-lab/biobert),,1024,Official,en,Open Source,2.5.0,"sentence, token",true,word_embeddings,embeddings,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_pubmed_large_cased_en_2.5.0_2.4_1590487739645.zip,,,biobert_pubmed_large_cased_en_2.5.0_2.4_1590487739645.zip,2.4,1590487739645,mds
Explain Clinical Doc CARP,,John Snow Labs,explain_clinical_doc_carp,2020-08-19,"[pipeline, en, licensed]","A pretrained pipeline with ner_clinical, assertion_dl, re_clinical and ner_posology. It will extract clinical and medication entities, assign assertion status and find relationships between clinical entities.",,"- ner_clinical
- assertion_dl
- re_clinical
- ner_posology",,Healthcare,en,Licensed,2.5.5,,,,pipeline,https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/explain_clinical_doc_carp_en_2.5.5_2.4_1597841630062.zip,,https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/11.Pretrained_Clinical_Pipelines.ipynb,explain_clinical_doc_carp_en_2.5.5_2.4_1597841630062.zip,2.4,1597841630062,mds
