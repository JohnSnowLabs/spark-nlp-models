{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, PackageLoader, select_autoescape, meta\n",
    "import pandas as pd, boto3, re, pytz, os\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "pd.set_option(\"display.max_rows\",1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/fernandrez/JSL/repos/spark-nlp/docs/_posts\"\n",
    "files = os.listdir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for fp in files:\n",
    "    state = \"\"\n",
    "    with open(f\"/home/fernandrez/JSL/repos/spark-nlp/docs/_posts/{fp}\", \"r\") as f:\n",
    "        text = f.readlines()\n",
    "        record = {\"file\": fp}\n",
    "        for line in text:\n",
    "            any_cond = line==\"---\\n\" or any([t in line for t in [\"##\", \"## Description\",\"## Source\",\"## How to use\"]])\n",
    "            if any_cond:\n",
    "                if line == \"---\":\n",
    "                    state = \"\" if state==\"---\" else \"---\"\n",
    "                else:\n",
    "                    state = line.replace(\"\\n\",\"\").strip()\n",
    "            else:\n",
    "                if state == \"---\":\n",
    "                    key_val = line.split(\":\")\n",
    "                    if len(key_val)==2:\n",
    "                        record[key_val[0].strip()] =  key_val[1].strip()\n",
    "                elif record.get(state, None) is None:\n",
    "                    record[state] = line\n",
    "                else:\n",
    "                    record[state] = record[state] + \"\\n\" + line\n",
    "        data.append(record)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': '2020-02-03-wikiner_6B_300_fr.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, fr, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_FR){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_FR.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_fr_2.4.0_2.4_1579717534654.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"fr\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"fr\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|fr|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org)'},\n",
       " {'file': '2020-03-04-ner_deid_enriched_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Deidentification NER (Enriched)',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_deid_enriched',\n",
       "  'date': '2020-03-04',\n",
       "  'tags': '[ner, en, deidentify, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'Deidentification NER (Enriched) is a Named Entity Recognition model that annotates text to find protected health information that may need to be deidentified. The entities it annotates are Age, City, Country, Date, Doctor, Hospital, Idnum, Medicalrecord, Organization, Patient, Phone, Profession, State, Street, Username, and Zip. Clinical NER is trained with the \\'embeddings_clinical\\' word embeddings model, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/healthcare/NER_DEMOGRAPHICS){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/NER_DEMOGRAPHICS.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_deid_enriched_en_2.4.2_2.4_1587513306751.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"ner_deid_enriched\", \"en\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_deid_enriched\", \"en\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_deid_enriched|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP for Healthcare 2.4.2+ |\\n\\n|License:|Licensed|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|en|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/](https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/)'},\n",
       " {'file': '2019-07-13-wikiner_840B_300_fr.md',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'layout': 'model',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2019-07-13',\n",
       "  'tags': '[open_source, ner, fr]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_FR){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_FR.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_fr_2.1.0_2.4_1563035043013.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"fr\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"fr\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.1.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|fr|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org)'},\n",
       " {'file': '2020-04-22-ner_jsl_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Ner DL Model',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_jsl_en',\n",
       "  'date': '2020-04-22',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for clinical terminology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Included Entities': 'Age, Diagnosis, Dosage, Drug_name, Frequency, Gender, Lab_name, Lab_result, Symptom_name\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_jsl_en_2.4.2_2.4_1587513304751.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_jsl\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_jsl\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_jsl_en_2.4.2_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.2|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': 'Trained on data gathered and manually annotated by John Snow Labs.\\n\\nhttps://www.johnsnowlabs.com/data/\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_jsl.png)'},\n",
       " {'file': '2019-07-13-wikiner_840B_300_de.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2019-07-13',\n",
       "  'tags': '[open_source, ner, de]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_DE){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_DE.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_de_2.1.0_2.4_1563035544700.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"de\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"de\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.1.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|de|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://de.wikipedia.org](https://de.wikipedia.org)'},\n",
       " {'file': '2020-05-10-ner_clinical_large_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Clinical NER (Large)',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_clinical_large',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'Clinical NER (Large) is a Named Entity Recognition model that annotates text to find references to clinical events. The entities it annotates are Problem, Treatment, and Test. Clinical NER is trained with the \\'embeddings_clinical\\' word embeddings model, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/healthcare/NER_EVENTS_CLINICAL){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/NER_EVENTS_CLINICAL.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_large_clinical_en_2.5.0_2.4_1590021302624.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"ner_clinical_large\", \"en\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_clinical_large\", \"en\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_clinical_large|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP for Healthcare 2.5.0+|\\n\\n|License:|Licensed|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|en|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/](https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/)'},\n",
       " {'file': '2020-03-16-wikiner_840B_300_ru.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2020-03-16',\n",
       "  'tags': '[ner, ru, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_RU){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_RU.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_ru_2.4.4_2.4_1584014001695.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"ru\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"ru\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.4|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|ru|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://ru.wikipedia.org](https://ru.wikipedia.org)'},\n",
       " {'file': '2020-01-30-ner_bionlp_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Bionlp',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_bionlp_en',\n",
       "  'date': '2020-01-30',\n",
       "  'tags': '[licensed, ner, en]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for biology and genetics terms. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': ' Amino_acid, Anatomical_system, Cancer, Cell, Cellular_component, Developing_anatomical_Structure, Gene_or_gene_product, Immaterial_anatomical_entity, Multi-tissue_structure, Organ, Organism, Organism_subdivision, Simple_chemical, Tissue\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_bionlp_en_2.4.0_2.4_1580237286004.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_bionlp\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_bionlp\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_bionlp_en_2.4.0_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on Cancer Genetics (CG) task of the BioNLP Shared Task 2013 with 'embeddings_clinical'.\\n\\nhttp://2013.bionlp-st.org/tasks/cancer-genetics\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_bionlp.png)'},\n",
       " {'file': '2020-04-28-albert_base_uncased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'ALBERT Base Uncase',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'albert_base_uncased',\n",
       "  'date': '2020-04-28',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'ALBERT is \"A Lite\" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper \"[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)\"\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_base_uncased_en_2.5.0_2.4_1588073363475.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = AlbertEmbeddings.pretrained(\"albert_base_uncased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = AlbertEmbeddings.pretrained(\"albert_base_uncased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|albert_base_uncased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|768|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/albert_base/3](https://tfhub.dev/google/albert_base/3)\\n'},\n",
       " {'file': '2020-05-10-wikiner_6B_100_pt.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 100',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_100',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, pt, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_PT){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PT.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_pt_2.5.0_2.4_1588495233192.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_100\", \"pt\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_100\", \"pt\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_100|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|pt|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://pt.wikipedia.org](https://pt.wikipedia.org)'},\n",
       " {'file': '2020-04-22-ner_posology_small_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Posology Small',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_posology_small_en',\n",
       "  'date': '2020-04-22',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for posology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': 'Dosage, Drug, Duration, Form, Frequency, Route, Strength\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazon.com/auxdata.johnsnowlabs.com/clinical/models/ner_posology_small_en_2.4.2_2.4_1587513301751.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_posology_small\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\nner_posology_small\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_posology_small\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_posology_small_en_2.4.2_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.2|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on the 2018 i2b2 dataset with 'embeddings_clinical'.\\n\\nhttps://www.i2b2.org/NLP/Medication\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline:\\n\\n\\n\\n![image](/assets/images/ner_posology.png) \\n'},\n",
       " {'file': '2020-02-03-glove_6B_300_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'GloVe 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'glove_6B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_EN){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.4.0_2.4_1579698630432.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = WordEmbeddingsModel.pretrained(\"glove_6B_300\", \"en\") \\\\\\n\\n        .setInputCols([\"document\", \"token\"]) \\\\\\n\\n        .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = WordEmbeddingsModel.pretrained(\"glove_6B_300\", \"en\")\\n\\n        .setInputCols(Array(\"document\", \"token\"))\\n\\n        .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|glove_6B_300|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence. token|\\n\\n|Output Labels:|embeddings|\\n\\n|Language:|en|\\n\\n|Dimension:|300|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)'},\n",
       " {'file': '2020-03-19-explain_document_dl.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Explain Document DL',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'explain_document_dl',\n",
       "  'tags': '[pipeline, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'The *explain_document_dl* is a pretrained pipeline that we can use to process text with a simple pipeline that performs basic processing steps.\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/explain-document-ml/explain_document_ml.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/explain_document_dl_en_2.4.3_2.4_1584626657780.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```scala\\n\\n\\n\\ncode example\\n\\n```\\n\\n\\n\\n```python\\n\\n\\n\\npipeline = PretrainedPipeline(\\'explain_document_dl\\', lang =\\' en\\').annotate(\\' Hello world!\\')\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|explain_document_dl|\\n\\n|Type:|pipeline|\\n\\n|Compatibility:|Spark NLP 2.5.5|\\n\\n|License:|Open Source|\\n\\n|Edition:|Community|\\n\\n|Language:|[en]|\\n\\n\\n\\n\\n',\n",
       "  '## Included Models': 'The explain_document_ml has one Transformer and six annotators: \\n\\n- Documenssembler - A Transformer that creates a column that contains documents. \\n\\n- Sentence Segmenter - An annotator that produces the sentences of the document. \\n\\n- Tokenizer - An annotator that produces the tokens of the sentences. \\n\\n- SpellChecker - An annotator that produces the spelling-corrected tokens. \\n\\n- Stemmer - An annotator that produces the stems of the tokens. \\n\\n- Lemmatizer - An annotator that produces the lemmas of the tokens. \\n\\n- POS Tagger - An annotator that produces the parts of speech of the associated tokens.\\n\\n\\n',\n",
       "  '## Results': '\\n\\n{:.result_box}\\n\\n```python\\n\\n\\n\\nresult = pipeline.annotate(testDoc, \"text\")\\n\\nresult.printSchema()\\n\\nresult.show()\\n\\n\\n\\nroot\\n\\n |-- text: string (nullable = true)\\n\\n |-- document: array (nullable = true)\\n\\n |    |-- element: struct (containsNull = true)\\n\\n |    |    |-- annotatorType: string (nullable = true)\\n\\n |    |    |-- begin: integer (nullable = false)\\n\\n |    |    |-- end: integer (nullable = false)\\n\\n |    |    |-- result: string (nullable = true)\\n\\n |    |    |-- metadata: map (nullable = true)\\n\\n |    |    |    |-- key: string\\n\\n |    |    |    |-- value: string (valueContainsNull = true)\\n\\n |    |    |-- embeddings: array (nullable = true)\\n\\n |    |    |    |-- element: float (containsNull = false)\\n\\n |-- sentence: array (nullable = true)\\n\\n |    |-- element: struct (containsNull = true)\\n\\n |    |    |-- annotatorType: string (nullable = true)\\n\\n |    |    |-- begin: integer (nullable = false)\\n\\n |    |    |-- end: integer (nullable = false)\\n\\n |    |    |-- result: string (nullable = true)\\n\\n |    |    |-- metadata: map (nullable = true)\\n\\n |    |    |    |-- key: string\\n\\n |    |    |    |-- value: string (valueContainsNull = true)\\n\\n |    |    |-- embeddings: array (nullable = true)\\n\\n |    |    |    |-- element: float (containsNull = false)\\n\\n |-- token: array (nullable = true)\\n\\n |    |-- element: struct (containsNull = true)\\n\\n |    |    |-- annotatorType: string (nullable = true)\\n\\n |    |    |-- begin: integer (nullable = false)\\n\\n |    |    |-- end: integer (nullable = false)\\n\\n |    |    |-- result: string (nullable = true)\\n\\n |    |    |-- metadata: map (nullable = true)\\n\\n |    |    |    |-- key: string\\n\\n |    |    |    |-- value: string (valueContainsNull = true)\\n\\n |    |    |-- embeddings: array (nullable = true)\\n\\n |    |    |    |-- element: float (containsNull = false)\\n\\n |-- spell: array (nullable = true)\\n\\n |    |-- element: struct (containsNull = true)\\n\\n |    |    |-- annotatorType: string (nullable = true)\\n\\n |    |    |-- begin: integer (nullable = false)\\n\\n |    |    |-- end: integer (nullable = false)\\n\\n |    |    |-- result: string (nullable = true)\\n\\n |    |    |-- metadata: map (nullable = true)\\n\\n |    |    |    |-- key: string\\n\\n |    |    |    |-- value: string (valueContainsNull = true)\\n\\n |    |    |-- embeddings: array (nullable = true)\\n\\n |    |    |    |-- element: float (containsNull = false)\\n\\n |-- lemmas: array (nullable = true)\\n\\n |    |-- element: struct (containsNull = true)\\n\\n |    |    |-- annotatorType: string (nullable = true)\\n\\n |    |    |-- begin: integer (nullable = false)\\n\\n |    |    |-- end: integer (nullable = false)\\n\\n |    |    |-- result: string (nullable = true)\\n\\n |    |    |-- metadata: map (nullable = true)\\n\\n |    |    |    |-- key: string\\n\\n |    |    |    |-- value: string (valueContainsNull = true)\\n\\n |    |    |-- embeddings: array (nullable = true)\\n\\n |    |    |    |-- element: float (containsNull = false)\\n\\n |-- stems: array (nullable = true)\\n\\n |    |-- element: struct (containsNull = true)\\n\\n |    |    |-- annotatorType: string (nullable = true)\\n\\n |    |    |-- begin: integer (nullable = false)\\n\\n |    |    |-- end: integer (nullable = false)\\n\\n |    |    |-- result: string (nullable = true)\\n\\n |    |    |-- metadata: map (nullable = true)\\n\\n |    |    |    |-- key: string\\n\\n |    |    |    |-- value: string (valueContainsNull = true)\\n\\n |    |    |-- embeddings: array (nullable = true)\\n\\n |    |    |    |-- element: float (containsNull = false)\\n\\n |-- pos: array (nullable = true)\\n\\n |    |-- element: struct (containsNull = true)\\n\\n |    |    |-- annotatorType: string (nullable = true)\\n\\n |    |    |-- begin: integer (nullable = false)\\n\\n |    |    |-- end: integer (nullable = false)\\n\\n |    |    |-- result: string (nullable = true)\\n\\n |    |    |-- metadata: map (nullable = true)\\n\\n |    |    |    |-- key: string\\n\\n |    |    |    |-- value: string (valueContainsNull = true)\\n\\n |    |    |-- embeddings: array (nullable = true)\\n\\n |    |    |    |-- element: float (containsNull = false)\\n\\n\\n\\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\\n\\n|                text|            document|            sentence|               token|               spell|              lemmas|               stems|                 pos|\\n\\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\\n\\n|French author who...|[[document, 0, 23...|[[document, 0, 57...|[[token, 0, 5, Fr...|[[token, 0, 5, Fr...|[[token, 0, 5, Fr...|[[token, 0, 5, fr...|[[pos, 0, 5, JJ, ...|\\n\\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\\n\\n\\n\\n```'},\n",
       " {'file': '2020-01-02-bert_large_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BERT Large Cased',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'bert_large_cased',\n",
       "  'date': '2020-01-02',\n",
       "  'tags': '[open_source, embeddings, en]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_large_cased_en_2.4.0_2.4_1580580251298.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"bert_large_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"bert_large_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|bert_large_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|1024|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16/1](https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16/1)\\n'},\n",
       " {'file': '2020-04-17-tfhub_use.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Universal Sentence Encoder',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'tfhub_use',\n",
       "  'date': '2020-04-17',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\\n\\n\\n\\nThe model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. We apply this model to the STS benchmark for semantic similarity, and the results can be seen in the example notebook made available. The universal-sentence-encoder model is trained with a deep averaging network (DAN) encoder.\\n\\n\\n\\nThe details are described in the paper \"[Universal Sentence Encoder](https://arxiv.org/abs/1803.11175)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/tfhub_use_en_2.4.0_2.4_1587136330099.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\") \\\\\\n\\n      .setInputCols(\"document\") \\\\\\n\\n      .setOutputCol(\"sentence_embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\")\\n\\n      .setInputCols(\"document\")\\n\\n      .setOutputCol(\"sentence_embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|tfhub_use|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence]|\\n\\n|Output Labels:|[sentence_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|512|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/universal-sentence-encoder/2](https://tfhub.dev/google/universal-sentence-encoder/2)\\n'},\n",
       " {'file': '2020-07-20-biobert_clinical_base_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BioBERT Clinical',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'biobert_clinical_base_cased',\n",
       "  'date': '2020-07-20',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a pre-trained weights of ClinicalBERT for generic clinical text. This domain-specific model has performance improvements on 3/5 clinical NLP tasks andd establishing a new state-of-the-art on the MedNLI dataset. The details are described in the paper \"[Publicly Available Clinical BERT Embeddings](https://www.aclweb.org/anthology/W19-1909/)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_clinical_base_cased_en_2.5.0_2.4_1590489819943.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"biobert_clinical_base_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"biobert_clinical_base_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|biobert_clinical_base_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|768|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://github.com/EmilyAlsentzer/clinicalBERT](https://github.com/EmilyAlsentzer/clinicalBERT)\\n'},\n",
       " {'file': '2020-08-19-explain_clinical_doc_era_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Explain Clinical Doc ERA',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'explain_clinical_doc_era_en',\n",
       "  'date': '2020-08-19',\n",
       "  'tags': '[pipeline, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'A pretrained pipeline with ner_clinical_events, assertion_dl and re_temporal_events_clinical. It will extract clinical entities, assign assertion status and find temporal relationships between clinical entities\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/11.Pretrained_Clinical_Pipelines.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/explain_clinical_doc_era_en_2.5.5_2.4_1597841630062.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\n\\n\\npipeline = PretrainedPipeline(\\'explain_clinical_doc_era\\', \\'en\\', \\'clinical/models\\')\\n\\n\\n\\nannotations = pipeline.annotate(text)\\n\\n\\n\\nannotations.keys()\\n\\n\\n\\n```\\n\\n```scala\\n\\n\\n\\n\\n\\npipeline = PretrainedPipeline(\\'explain_clinical_doc_era\\', \\'en\\', \\'clinical/models\\')\\n\\n\\n\\nannotations = pipeline.annotate(text)\\n\\n\\n\\nannotations.keys()\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|explain_clinical_doc_era_en_2.5.5_2.4|\\n\\n|Type:|pipeline|\\n\\n|Compatibility:|Spark NLP 2.5.5|\\n\\n|License:|Licensed|\\n\\n|Edition:|Healthcare|\\n\\n|Language:|[en]|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Included Models': ' - ner_clinical_events\\n\\n - assertion_dl\\n\\n - re_temporal_events_clinical\\n\\n \\n\\n{:.h2_title}\\n',\n",
       "  '## Results': \"\\n\\nTHe output is a dictionary with the following keys: 'sentences', 'clinical_ner_tags', 'clinical_ner_chunks_re', 'document', 'clinical_ner_chunks', 'assertion', 'clinical_relations', 'tokens', 'embeddings', 'pos_tags', 'dependencies'.\\n\"},\n",
       " {'file': '2020-05-10-wikiner_6B_300_pl.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, pl, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_PL){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PL.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_pl_2.5.0_2.4_1588519719571.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"pl\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"pl\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|pl|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://pl.wikipedia.org](https://pl.wikipedia.org)'},\n",
       " {'file': '2020-01-02-bert_base_uncased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BERT Base Uncased',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'bert_base_uncased',\n",
       "  'date': '2020-01-02',\n",
       "  'tags': '[open_source, embeddings, en]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_en_2.4.0_2.4_1580579557778.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"bert_base_uncased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"bert_base_uncased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|bert_base_uncased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|768|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1](https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1)\\n'},\n",
       " {'file': '2020-04-28-xlnet_large_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'XLNet Large',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'xlnet_large_cased',\n",
       "  'date': '2020-04-28',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context. Overall, XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering, natural language inference, sentiment analysis, and document ranking. The details are described in the paper \"[\\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)\"\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlnet_large_cased_en_2.5.0_2.4_1588074397954.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = XlnetEmbeddings.pretrained(\"xlnet_large_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = XlnetEmbeddings.pretrained(\"xlnet_large_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|xlnet_large_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|1024|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet)'},\n",
       " {'file': '2020-05-23-ner_clinical_large_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Clinical Large',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_clinical_large_en',\n",
       "  'date': '2020-05-23',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for clinical terms. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Included Entities': 'Problem, Test, Treatment\\n\\n\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_clinical_large_en_2.5.0_2.4_1590021302624.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_clinical_large\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\nval ner = NerDLModel.pretrained(\"ner_clinical_large\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_clinical_large_en_2.5.0_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licenced|\\n\\n|Input Labels:|[sentence, token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on augmented 2010 i2b2 challenge data with 'embeddings_clinical'.\\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe:\\n\\n\\n\\n![image](/assets/images/ner_clinical.png)'},\n",
       " {'file': '2020-05-10-wikiner_840B_300_pl.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, pl, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_PL){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PL.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_pl_2.5.0_2.4_1588519719572.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"pl\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"pl\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|pl|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://pl.wikipedia.org](https://pl.wikipedia.org)'},\n",
       " {'file': '2020-02-03-wikiner_6B_300_es.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, es, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_ES){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_ES.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_es_2.4.0_2.4_1581971942090.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"es\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"es\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|es|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://es.wikipedia.org](https://es.wikipedia.org)'},\n",
       " {'file': '2020-01-02-bert_large_uncased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BERT Large Uncased',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'bert_large_uncased',\n",
       "  'date': '2020-01-02',\n",
       "  'tags': '[open_source, embeddings, en]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_large_uncased_en_2.4.0_2.4_1580581306683.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"bert_large_uncased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"bert_large_uncased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|bert_large_uncased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|1024|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1](https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1)\\n'},\n",
       " {'file': '2020-03-04-ner_deid_large_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Deidentification NER (Large)',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_deid_large',\n",
       "  'date': '2020-03-04',\n",
       "  'tags': '[ner, en, deidentify, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'Deidentification NER (Large) is a Named Entity Recognition model that annotates text to find protected health information that may need to be deidentified. The entities it annotates are Age, Contact, Date, Id, Location, Name, and Profession. Clinical NER is trained with the \\'embeddings_clinical\\' word embeddings model, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/healthcare/NER_DEMOGRAPHICS){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/NER_DEMOGRAPHICS.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_deid_large_en_2.4.2_2.4_1587513305751.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"ner_deid_large\", \"en\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_deid_large\", \"en\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_deid_large|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP for Healthcare 2.4.2+|\\n\\n|License:|Licensed|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|en|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/](https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/)'},\n",
       " {'file': '2020-03-25-ner_drugs_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Drugs',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_drugs_en',\n",
       "  'date': '2020-03-25',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for Drugs. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Included Entities': ' - DrugChem\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_drugs_en_2.4.4_2.4_1584452534235.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_drugs\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_drugs\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_drugs_en_2.4.4_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.4|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on i2b2_med7 + FDA with 'embeddings_clinical'.\\n\\nhttps://www.i2b2.org/NLP/Medication\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_drugs.png)'},\n",
       " {'file': '2020-02-03-wikiner_840B_300_es.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, es, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_ES){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_ES.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_es_2.4.0_2.4_1581971942091.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"es\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"es\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|es|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://es.wikipedia.org](https://es.wikipedia.org)'},\n",
       " {'file': '2020-01-22-glove_840B_300.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Glove 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'glove_840B_300',\n",
       "  'date': '2020-01-22',\n",
       "  'tags': '[open_source, embeddings]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_840B_300_xx_2.4.0_2.4_1579698926752.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = WordEmbeddings.pretrained(\"glove_840B_300\", \"xx\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = WordEmbeddings.pretrained(\"glove_840B_300\", \"xx\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|glove_840B_300|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[xx]|\\n\\n|Dimension:|300|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)\\n'},\n",
       " {'file': '2020-05-21-assertion_dl_large_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Assertion DL Large',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'assertion_dl_large_en',\n",
       "  'date': '2020-05-21',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nDeep learning named entity recognition model for assertions. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN.\\n\\n\\n',\n",
       "  '## Included Assertions': '\\n\\n - hypothetical\\n\\n - present\\n\\n - absent\\n\\n - possible\\n\\n - conditional\\n\\n - associated_with_someone_else\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.Clinical_Assertion_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/assertion_dl_large_en_2.5.0_2.4_1590022282256.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel, NerConverter, AssertionDLModel.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_assertion = AssertionDLModel.pretrained(\"assertion_dl_large\", \"en\", \"clinical/models\") \\\\\\n\\n    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\\\\n\\n    .setOutputCol(\"assertion\")\\n\\n    \\n\\nnlpPipeline = Pipeline(stages=[clinical_assertion])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval clinical_assertion = AssertionDLModel.pretrained(\"assertion_dl_large\", \"en\", \"clinical/models\") \\\\\\n\\n    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\\\\n\\n    .setOutputCol(\"assertion\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(clinical_assertion))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|assertion_dl_large_en_2.5.0_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence, ner_chunk, embeddings]|\\n\\n|Output Labels:|[assertion]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text with 'embeddings_clinical'.\\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and an \"assertion\" column containing all of the assertion labels in the sentence. The assertion column also contains assertion character indices, and other metadata. To get only the entity chunks and assertion labels, without the metadata, select \"ner_chunk.result\" and \"assertion.result\" from your output dataframe.\\n\\n\\n\\n![image](/assets/images/assertiondl.png)\\n'},\n",
       " {'file': '2020-04-28-xlnet_base_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'XLNet Base',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'xlnet_base_cased',\n",
       "  'date': '2020-04-28',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context. Overall, XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering, natural language inference, sentiment analysis, and document ranking. The details are described in the paper \"[\\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)\"\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlnet_base_cased_en_2.5.0_2.4_1588074114942.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = XlnetEmbeddings.pretrained(\"xlnet_base_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = XlnetEmbeddings.pretrained(\"xlnet_base_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|xlnet_base_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|768|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet)\\n'},\n",
       " {'file': '2020-04-28-albert_xlarge_uncased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'ALBERT XLarge Uncase',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'albert_xlarge_uncased',\n",
       "  'date': '2020-04-28',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'ALBERT is \"A Lite\" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper \"[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)\"\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_xlarge_uncased_en_2.5.0_2.4_1588073443653.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = AlbertEmbeddings.pretrained(\"albert_xlarge_uncased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = AlbertEmbeddings.pretrained(\"albert_xlarge_uncased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|albert_xlarge_uncased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|2048|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/albert_xlarge/3](https://tfhub.dev/google/albert_xlarge/3)\\n'},\n",
       " {'file': '2020-04-28-albert_xxlarge_uncased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'ALBERT XXLarge Uncase',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'albert_xxlarge_uncased',\n",
       "  'date': '2020-04-28',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'ALBERT is \"A Lite\" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper \"[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)\"\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_xxlarge_uncased_en_2.5.0_2.4_1588073588232.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = AlbertEmbeddings.pretrained(\"albert_xxlarge_uncased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = AlbertEmbeddings.pretrained(\"albert_xxlarge_uncased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|albert_xxlarge_uncased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|1024|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/albert_xlarge/3](https://tfhub.dev/google/albert_xlarge/3)\\n'},\n",
       " {'file': '2019-07-13-wikiner_6B_100_de.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Wiki NER 6B 100',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_100',\n",
       "  'date': '2019-07-13',\n",
       "  'tags': '[open_source, ner, de]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'Wiki NER is a Named Entity Recognition (or NER) model, that can be used to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. Wiki NER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_DE){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_DE.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_de_2.1.0_2.4_1564861417829.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_100\", \"de\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_100\", \"de\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_100|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.1.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|de|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is trained based on data from [https://de.wikipedia.org](https://de.wikipedia.org)'},\n",
       " {'file': '2020-01-02-bert_base_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BERT Base Cased',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'bert_base_cased',\n",
       "  'date': '2020-01-02',\n",
       "  'tags': '[open_source, embeddings, en]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_en_2.4.0_2.4_1580579557778.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"bert_base_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"bert_base_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|bert_base_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|768|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16/1](https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16/1)\\n'},\n",
       " {'file': '2020-02-03-wikiner_840B_300_it.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, it, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_IT){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_IT.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_it_2.4.0_2.4_1579699913554.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"it\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"it\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|it|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://it.wikipedia.org](https://it.wikipedia.org)'},\n",
       " {'file': '2020-03-16-wikiner_6B_300_ru.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2020-03-16',\n",
       "  'tags': '[ner, ru, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_RU){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_RU.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_ru_2.4.4_2.4_1584014001694.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"ru\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"ru\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.4|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|ru|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://ru.wikipedia.org](https://ru.wikipedia.org)'},\n",
       " {'file': '2020-04-28-albert_large_uncased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'ALBERT Large Uncase',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'albert_large_uncased',\n",
       "  'date': '2020-04-28',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'ALBERT is \"A Lite\" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper \"[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)\"\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_large_uncased_en_2.5.0_2.4_1588073397355.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = AlbertEmbeddings.pretrained(\"albert_large_uncased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = AlbertEmbeddings.pretrained(\"albert_large_uncased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|albert_large_uncased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|1024|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/albert_large/3](https://tfhub.dev/google/albert_large/3)\\n'},\n",
       " {'file': '2020-01-30-assertion_dl_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Assertion DL',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'assertion_dl_en',\n",
       "  'date': '2020-01-30',\n",
       "  'tags': '[licensed, ner, en]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nDeep learning named entity recognition model for assertions. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN.\\n\\n\\n',\n",
       "  '## Included Assertions': '\\n\\n Hypothetical, Present, Absent, Possible, Conditional, Associated_with_someone_else \\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.Clinical_Assertion_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/assertion_dl_en_2.4.0_2.4_1580237286004.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel, NerConverter, AssertionDLModel.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\nclinical_assertion = AssertionDLModel.pretrained(\"assertion_dl\", \"en\", \"clinical/models\") \\\\\\n\\n    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\\\\n\\n    .setOutputCol(\"assertion\")\\n\\n    \\n\\nnlpPipeline = Pipeline(stages=[clinical_assertion])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n```\\n\\n\\n\\n```scala\\n\\nval clinical_assertion = AssertionDLModel.pretrained(\"assertion_dl\", \"en\", \"clinical/models\") \\\\\\n\\n    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\\\\n\\n    .setOutputCol(\"assertion\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(clinical_assertion))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|assertion_dl_en_2.4.0_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence, ner_chunk, embeddings]|\\n\\n|Output Labels:|[assertion]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text with 'embeddings_clinical'.\\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and an \"assertion\" column containing all of the assertion labels in the sentence. The assertion column also contains assertion character indices, and other metadata. To get only the entity chunks and assertion labels, without the metadata, select \"ner_chunk.result\" and \"assertion.result\" from your output dataframe.\\n\\n\\n\\n![image](/assets/images/assertiondl.png) \\n'},\n",
       " {'file': '2020-03-26-ner_healthcare_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Healthcare',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_healthcare_en',\n",
       "  'date': '2020-03-26',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for healthcare. Includes Problem, Test and Treatment entities. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': ' Problem, Test, Treatment\\n\\n\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_healthcare_en_2.4.4_2.4_1585188313964.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_healthcare\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_healthcare\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_healthcare_en_2.4.4_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.4|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on 2010 i2b2 challenge data with 'embeddings_clinical'.\\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n![image](/assets/images/ner_clinical.png)'},\n",
       " {'file': '2019-07-13-wikiner_6B_300_fr.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2019-07-13',\n",
       "  'tags': '[open_source, ner, fr]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_FR){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_FR.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_fr_2.1.0_2.4_1564817386216.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"fr\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"fr\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.1.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|fr|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org)'},\n",
       " {'file': '2020-04-22-ner_risk_factors_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Risk Factors',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_risk_factors_en',\n",
       "  'date': '2020-04-22',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for Heart Disease Risk Factors and Personal Health Information. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_risk_factors_en_2.4.2_2.4_1587513300751.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': 'CAD, Diabetes, Family_hist, Hyperlipidemia, Hypertension, Medication, Obese, PHI, Smoker\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_risk_factors\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_risk_factors\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_risk_factors_en_2.4.2_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.2|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on plain n2c2 2014: De-identification and Heart Disease Risk Factors Challenge datasets with 'embeddings_clinical'.\\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_risks.png)'},\n",
       " {'file': '2020-02-03-wikiner_840B_300_de.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, de, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_DE){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_DE.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_de_2.4.0_2.4_1579699913555.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"de\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"de\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|de|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://de.wikipedia.org](https://de.wikipedia.org)'},\n",
       " {'file': '2020-08-04-deidentify_large_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Deidentify (Large)',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'deidentify_large',\n",
       "  'date': '2020-08-04',\n",
       "  'tags': '[deid, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'Deidentify (Large) is a deidentification model. It identifies instances of protected health information in text documents, and it can either obfuscate them (e.g., replacing names with different, fake names) or mask them (e.g., replacing \"2020-06-04\" with \"<DATE>\"). This model is useful for maintaining HIPAA compliance when dealing with text documents that contain protected health information.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/healthcare/DEID_PHI_TEXT){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/DEID_PHI_TEXT.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/nerdl_deid_en_1.8.0_2.4_1545462443516.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\ndeid = DeIdentificationModel.pretrained(\"deidentify_large\", \"en\") \\\\\\n\\n        .setInputCols([\"sentence\", \"token\", \"ner_chunk\"]) \\\\\\n\\n        .setOutputCol(\"obfuscated\") \\\\\\n\\n          .setMode(\"obfuscate\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval deid = DeIdentificationModel.pretrained(\"deidentify_large\", \"en\")\\n\\n        .setInputCols(Array(\"sentence\", \"token\", \"ner_chunk\"))\\n\\n        .setOutputCol(\"obfuscated\") \\\\\\n\\n          .setMode(\"obfuscate\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|deidentify_large|\\n\\n|Type:|deid|\\n\\n|Compatibility:| Spark NLP for Healthcare 2.5.5|\\n\\n|License:|Licensed|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, ner_chunk|\\n\\n|Output Labels:|obfuscated|\\n\\n|Language:|en|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/](https://portal.dbmi.hms.harvard.edu/projects/n2c2-2014/)'},\n",
       " {'file': '2020-01-30-assertion_ml_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Assertion ML',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'assertion_ml_en',\n",
       "  'date': '2020-01-30',\n",
       "  'tags': '[licensed, ner, en]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nLogistic regression based named entity recognition model for assertions. \\n\\n\\n',\n",
       "  '## Included Assertions': '\\n\\n Hypothetical, Present, Absent, Possible, Conditional, Associated_with_someone_else \\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.Clinical_Assertion_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/assertion_dl_large_en_2.5.0_2.4_1590022282256.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel, NerConverter, AssertionLogRegModel.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_assertion_ml = AssertionLogRegModel.pretrained(\"assertion_ml\", \"en\", \"clinical/models\") \\\\\\n\\n    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\\\\n\\n    .setOutputCol(\"assertion\")\\n\\n    \\n\\nnlpPipeline = Pipeline(stages=[clinical_assertion_ml])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval clinical_assertion_ml = AssertionLogRegModel.pretrained(\"assertion_ml\", \"en\", \"clinical/models\") \\\\\\n\\n    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\\\\n\\n    .setOutputCol(\"assertion\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(clinical_assertion_ml))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|assertion_ml_en_2.4.0_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence, ner_chunk, embeddings]|\\n\\n|Output Labels:|[assertion]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text with 'embeddings_clinical'.\\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and an \"assertion\" column containing all of the assertion labels in the sentence. The assertion column also contains assertion character indices, and other metadata. To get only the entity chunks and assertion labels, without the metadata, select \"ner_chunk.result\" and \"assertion.result\" from your output dataframe.\\n\\n\\n\\n![image](/assets/images/assertiondl.png) \\n'},\n",
       " {'file': '2020-02-03-glove_840B_300_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'GloVe 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'glove_840B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_EN){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_840B_300_xx_2.4.0_2.4_1579698926752.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = WordEmbeddingsModel.pretrained(\"glove_840B_300\", \"en\") \\\\\\n\\n        .setInputCols([\"document\", \"token\"]) \\\\\\n\\n        .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = WordEmbeddingsModel.pretrained(\"glove_840B_300\", \"en\")\\n\\n        .setInputCols(Array(\"document\", \"token\"))\\n\\n        .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|glove_840B_300|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence. token|\\n\\n|Output Labels:|embeddings|\\n\\n|Language:|en|\\n\\n|Dimension:|300|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)'},\n",
       " {'file': '2020-05-10-wikiner_6B_100_nl.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 100',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_100',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, nl, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_NL){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_NL.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_nl_2.5.0_2.4_1588546201140.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_100\", \"nl\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_100\", \"nl\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_100|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|nl|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org)'},\n",
       " {'file': '2020-04-22-ner_jsl_enriched_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Ner DL Model Enriched',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_jsl_enriched_en',\n",
       "  'date': '2020-04-22',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for clinical terminology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': 'Age, Diagnosis, Dosage, Drug_name, Frequency, Gender, Lab_name, Lab_result, Symptom_name\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_jsl_enriched_en_2.4.2_2.4_1587513303751.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_jsl_enriched\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_jsl_enriched\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_jsl_enriched_en_2.4.2_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.2|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': 'Trained on data gathered and manually annotated by John Snow Labs.\\n\\nhttps://www.johnsnowlabs.com/data/\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_jsl.png)'},\n",
       " {'file': '2019-07-13-wikiner_840B_300_it.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2019-07-13',\n",
       "  'tags': '[open_source, ner, it]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_IT){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_IT.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_it_2.1.0_2.4_1563095099139.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"it\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"it\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.1.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|it|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://it.wikipedia.org](https://it.wikipedia.org)'},\n",
       " {'file': '2020-01-22-glove_100d.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Glove 6B 100',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'glove_100d',\n",
       "  'date': '2020-01-22',\n",
       "  'tags': '[open_source, embeddings, en]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_EN/){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_dl.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.4.0_2.4_1579690104032.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = WordEmbeddingsModel.pretrained(\"glove_100d\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|glove_100d|\\n\\n|Type:|word_embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|100|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)\\n'},\n",
       " {'file': '2020-03-25-ner_events_clinical_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Clinical Events',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_events_clinical',\n",
       "  'date': '2020-03-25',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for clinical events. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': 'Date,Time,Problem,Test,Treatment,Occurence,Clinical_Dept,Evidential,Duration,Frequency,Admission,Discharge\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_events_clinical_en_2.5.0_2.4_1590021303624.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_events_clinical\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_events_clinical\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_events_clinical_en_2.5.0_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on i2b2 events data with 'clinical_embeddings'. \\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_clinical.png) '},\n",
       " {'file': '2020-07-03-ner_neoplasms_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Neoplasms NER',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_neoplasms',\n",
       "  'date': '2020-07-03',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'Neoplasms NER is a Named Entity Recognition model that annotates text to find references to tumors. The only entity it annotates is MalignantNeoplasm. Neoplasms NER is trained with the \\'embeddings_clinical\\' word embeddings model, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/healthcare/NER_TUMOR){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/NER_TUMOR.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_neoplasms_es_2.5.3_2.4_1594168624415.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"ner_neoplasms\", \"en\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_neoplasms\", \"en\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_neoplasms|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP for Healthcare 2.5.3+|\\n\\n|License:|Licensed|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|en|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://temu.bsc.es/cantemist/](https://temu.bsc.es/cantemist/)'},\n",
       " {'file': '2020-07-20-biobert_discharge_base_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BioBERT Discharge',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'biobert_discharge_base_cased',\n",
       "  'date': '2020-07-20',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a pre-trained weights of ClinicalBERT for discharge summaries. This domain-specific model has performance improvements on 3/5 clinical NLP tasks andd establishing a new state-of-the-art on the MedNLI dataset. The details are described in the paper \"[Publicly Available Clinical BERT Embeddings](https://www.aclweb.org/anthology/W19-1909/)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_discharge_base_cased_en_2.5.0_2.4_1590490193605.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"biobert_discharge_base_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"biobert_discharge_base_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|biobert_discharge_base_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|768|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://github.com/EmilyAlsentzer/clinicalBERT](https://github.com/EmilyAlsentzer/clinicalBERT)\\n'},\n",
       " {'file': '2020-02-03-wikiner_6B_300_de.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, de, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_DE){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_DE.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_de_2.4.0_2.4_1579717534653.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"de\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"de\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|de|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://de.wikipedia.org](https://de.wikipedia.org)'},\n",
       " {'file': '2020-07-20-biobert_pmc_base_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BioBERT PMC',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'biobert_pmc_base_cased',\n",
       "  'date': '2020-07-20',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a pre-trained weights of BioBERT, a language representation model for biomedical domain, especially designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. The details are described in the paper \"[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_pmc_base_cased_en_2.5.0_2.4_1590489029151.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"biobert_pmc_base_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"biobert_pmc_base_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|biobert_pmc_base_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|768|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://github.com/dmis-lab/biobert](https://github.com/dmis-lab/biobert)\\n'},\n",
       " {'file': '2020-03-16-wikiner_6B_100_ru.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 100',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_100',\n",
       "  'date': '2020-03-16',\n",
       "  'tags': '[ner, ru, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_RU){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_RU.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_ru_2.4.4_2.4_1584014001452.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_100\", \"ru\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_100\", \"ru\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_100|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.4|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|ru|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://ru.wikipedia.org](https://ru.wikipedia.org)'},\n",
       " {'file': '2020-05-10-wikiner_6B_300_nl.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, nl, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_NL){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_NL.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_nl_2.5.0_2.4_1588546201483.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"nl\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"nl\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|nl|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org)'},\n",
       " {'file': '2020-02-03-wikiner_840B_300_fr.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, fr, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_FR){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_FR.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_fr_2.4.0_2.4_1579699913554.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"fr\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"fr\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|fr|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org)'},\n",
       " {'file': '2020-05-10-wikiner_840B_300_nl.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, nl, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_NL){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_NL.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_nl_2.5.0_2.4_1588546201484.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"nl\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"nl\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|nl|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is trained based on data from [https://fr.wikipedia.org](https://fr.wikipedia.org)'},\n",
       " {'file': '2020-05-10-wikiner_840B_300_pt.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 840B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_840B_300',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, pt, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 840B 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_PT){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PT.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_840B_300_pt_2.5.0_2.4_1588495233642.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_840B_300\", \"pt\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_840B_300\", \"pt\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_840B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.5.0|\\n\\n|Edition:|Official|\\n\\n|License:|Open Source|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|pt|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://pt.wikipedia.org](https://pt.wikipedia.org)'},\n",
       " {'file': '2020-01-31-elmo.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Elmo',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'elmo',\n",
       "  'date': '2020-01-31',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'Computes contextualized word representations using character-based word representations and bidirectional LSTMs.\\n\\n\\n\\nThis model outputs fixed embeddings at each LSTM layer and a learnable aggregation of the 3 layers.\\n\\n\\n\\n* `word_emb`: the character-based word representations with shape [batch_size, max_length, 512].  == word_emb\\n\\n* `lstm_outputs1`: the first LSTM hidden state with shape [batch_size, max_length, 1024]. === lstm_outputs1\\n\\n* `lstm_outputs2`: the second LSTM hidden state with shape [batch_size, max_length, 1024]. === lstm_outputs2\\n\\n* `elmo`: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape [batch_size, max_length, 1024]  == elmo\\n\\n  \\n\\nThe complex architecture achieves state of the art results on several benchmarks. Note that this is a very computationally expensive module compared to word embedding modules that only perform embedding lookups. The use of an accelerator is recommended.\\n\\n\\n\\nThe details are described in the paper \"[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/elmo_en_2.4.0_2.4_1580488815299.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = ElmoEmbeddings.pretrained(\"elmo\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\") \\\\\\n\\n      .setPoolingLayer(\"elmo\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = ElmoEmbeddings.pretrained(\"elmo\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n      .setPoolingLayer(\"elmo\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|elmo|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|512|1024|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/elmo/3](https://tfhub.dev/google/elmo/3)\\n'},\n",
       " {'file': '2020-02-03-onto_100_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Onto 100',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'onto_100',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'Onto is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. Onto was trained on the OntoNotes text corpus. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. Onto 100 is trained with GloVe 100d word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_EN_18){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/onto_100_en_2.4.0_2.4_1579729071672.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"onto_100\", \"en\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"onto_100\", \"en\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|onto_100|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|en|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)'},\n",
       " {'file': '2020-04-22-ner_posology_large_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Posology Large',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_posology_large_en',\n",
       "  'date': '2020-04-22',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for posology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': 'Dosage, Drug, Duration, Form, Frequency, Route, Strength\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_posology_large_en_2.4.2_2.4_1587513302751.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_posology_large\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_posology_large\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_posology_large_en_2.4.2_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.2|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on the 2018 i2b2 dataset and FDA Drug datasets with 'embeddings_clinical'.\\n\\nhttps://open.fda.gov/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_posology.png)'},\n",
       " {'file': '2020-03-25-ner_diseases_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Diseases',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_diseases_en',\n",
       "  'date': '2020-03-25',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for diseases. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Included Entities': ' - Disease\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](#){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_diseases_en_2.4.4_2.4_1584452534235.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_diseases\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_diseases\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n}\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_diseases_en_2.4.4_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.4|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on i2b2 with 'embeddings_clinical'.\\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe, or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_diseases.png)'},\n",
       " {'file': '2020-08-09-re_clinical_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'RelationExtractionModel Clinical',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 're_clinical_en',\n",
       "  'date': '2020-08-09',\n",
       "  'tags': '[re, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nModels the set of clinical relations defined in the 2010 i2b2 relation challenge.\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Included Relations': '\\n\\nTrIP: A certain treatment has improved or cured a medical problem (eg, infection resolved with antibiotic course)\\n\\n\\n\\nTrWP: A patient\\'s medical problem has deteriorated or worsened because of or in spite of a treatment being administered (eg, the tumor was growing despite the drain)\\n\\n\\n\\nTrCP: A treatment caused a medical problem (eg, penicillin causes a rash)\\n\\n\\n\\nTrAP: A treatment administered for a medical problem (eg, Dexamphetamine for narcolepsy)\\n\\n\\n\\nTrNAP: The administration of a treatment was avoided because of a medical problem (eg, Ralafen which is contra-indicated because of ulcers)\\n\\n\\n\\nTeRP: A test has revealed some medical problem (eg, an echocardiogram revealed a pericardial effusion)\\n\\n\\n\\nTeCP: A test was performed to investigate a medical problem (eg, chest x-ray done to rule out pneumonia)\\n\\n\\n\\nPIP: Two problems are related to each other (eg, Azotemia presumed secondary to sepsis)\\n\\n\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/10.Clinical_Relation_Extraction.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/re_clinical_en_2.5.5_2.4_1596928426753.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, PerceptronModel, NerDLModel, NerConverter, DependencyParserModel, RelationExtractionModel.\\n\\n\\n\\nThe precision of the RE model is controlled by \"setMaxSyntacticDistance(4)\", which sets the maximum syntactic distance between named entities to 4. A larger value will improve recall at the expense at lower precision.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\n\\n\\nclinical_re_Model = RelationExtractionModel()\\\\\\n\\n    .pretrained(\"re_clinical\", \"en\", \\'clinical/models\\')\\\\\\n\\n    .setInputCols([\"embeddings\", \"pos_tags\", \"ner_chunks\", \"dependencies\"])\\\\\\n\\n    .setOutputCol(\"relations\")\\\\\\n\\n    .setMaxSyntacticDistance(4)\\\\\\n\\n    .setRelationPairs([\"problem-test\", \"problem-treatment\"]) # Possible relation pairs. Default is all relations.\\n\\n\\n\\nloaded_pipeline = Pipeline(stages=[clinical_re_Model])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nloaded_model = loaded_pipeline.fit(empty_data)\\n\\n\\n\\nloaded_lmodel = LightPipeline(loaded_model)\\n\\n\\n\\nannotations = loaded_lmodel.fullAnnotate(text)\\n\\n\\n\\nrel_df = get_relations_df (annotations)\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|re_clinical_en_2.5.5_2.4|\\n\\n|Type:|re|\\n\\n|Compatibility:|Spark NLP 2.5.5|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[embeddings, pos_tags, ner_chunks, dependencies]|\\n\\n|Output Labels:|[relations]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on augmented 2010 i2b2 challenge data with 'clinical_embeddings'.\\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a Relation column and a Confidence column.\\n\\n![image](/assets/images/re_clinical.png)\\n'},\n",
       " {'file': '2020-02-03-onto_300_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Onto 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'onto_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'Onto is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. Onto was trained on the OntoNotes text corpus. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. Onto 300 is trained with GloVe 840B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_EN_18){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/onto_300_en_2.4.0_2.4_1579729071854.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"onto_300\", \"en\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"onto_300\", \"en\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|onto_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|en|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)'},\n",
       " {'file': '2020-05-10-wikiner_6B_100_pl.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 100',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_100',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, pl, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_PL){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PL.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_pl_2.5.0_2.4_1588519719293.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_100\", \"pl\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_100\", \"pl\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_100|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|pl|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://pl.wikipedia.org](https://pl.wikipedia.org)'},\n",
       " {'file': '2020-01-30-ner_clinical_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Clinical',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_clinical_en',\n",
       "  'date': '2020-01-30',\n",
       "  'tags': '[licensed, ner, en]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for clinical terms. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN.\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': ' Problem, Test, Treatment\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_clinical_en_2.4.0_2.4_1580237286004.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_clinical_en_2.4.0_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': 'Trained on 2010 i2b2 challenge data with `embeddings_clinical`.\\n\\nhttps://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_clinical.png)\\n'},\n",
       " {'file': '2020-07-20-biobert_pubmed_base_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BioBERT Pubmed',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'biobert_pubmed_base_cased',\n",
       "  'date': '2020-07-20',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a pre-trained weights of BioBERT, a language representation model for biomedical domain, especially designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. The details are described in the paper \"[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_pubmed_base_cased_en_2.5.0_2.4_1590487367971.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"biobert_pubmed_base_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"biobert_pubmed_base_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|biobert_pubmed_base_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|768|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://github.com/dmis-lab/biobert](https://github.com/dmis-lab/biobert)\\n'},\n",
       " {'file': '2020-04-15-ner_posology_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Posology',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_posology_en',\n",
       "  'date': '2020-04-15',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for posology. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': 'Dosage,Drug,Duration,Form,Frequency,Route,Strength\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_posology_en_2.4.4_2.4_1584452534235.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_posology\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_posology\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_posology_en_2.4.2_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.2|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on the 2018 i2b2 dataset and FDA Drug datasets with 'embeddings_clinical'.\\n\\nhttps://open.fda.gov/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_posology.png)\\n'},\n",
       " {'file': '2020-07-20-biobert_pubmed_pmc_base_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BioBERT Pubmed PMC',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'biobert_pubmed_pmc_base_cased',\n",
       "  'date': '2020-07-20',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a pre-trained weights of BioBERT, a language representation model for biomedical domain, especially designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. The details are described in the paper \"[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_pubmed_pmc_base_cased_en_2.5.0_2.4_1590489367180.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"biobert_pubmed_pmc_base_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"biobert_pubmed_pmc_base_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|biobert_pubmed_pmc_base_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|768|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://github.com/dmis-lab/biobert](https://github.com/dmis-lab/biobert)\\n'},\n",
       " {'file': '2020-04-17-tfhub_use_lg.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Universal Sentence Encoder Large',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'tfhub_use_lg',\n",
       "  'date': '2020-04-17',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\\n\\n\\n\\nThe model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. We apply this model to the STS benchmark for semantic similarity, and the results can be seen in the example notebook made available. The universal-sentence-encoder model is trained with a deep averaging network (DAN) encoder.\\n\\n\\n\\nThe details are described in the paper \"[Universal Sentence Encoder](https://arxiv.org/abs/1803.11175)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/tfhub_use_lg_en_2.4.0_2.4_1587136993894.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = UniversalSentenceEncoder.pretrained(\"tfhub_use_lg\", \"en\") \\\\\\n\\n      .setInputCols(\"document\") \\\\\\n\\n      .setOutputCol(\"sentence_embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = UniversalSentenceEncoder.pretrained(\"tfhub_use_lg\", \"en\")\\n\\n      .setInputCols(\"document\")\\n\\n      .setOutputCol(\"sentence_embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|tfhub_use_lg|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence]|\\n\\n|Output Labels:|[sentence_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|512|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://tfhub.dev/google/universal-sentence-encoder-large/3](https://tfhub.dev/google/universal-sentence-encoder-large/3)\\n'},\n",
       " {'file': '2020-01-22-glove_6B_300.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Glove 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'glove_6B_300',\n",
       "  'date': '2020-01-22',\n",
       "  'tags': '[open_source, embeddings]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'GloVe (Global Vectors) is a model for distributed word representation. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. It outperformed many common Word2vec models on the word analogy task. One benefit of GloVe is that it is the result of directly modeling relationships, instead of getting them as a side effect of training a language model.\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.4.0_2.4_1579698630432.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = WordEmbeddings.pretrained(\"glove_6B_300\", \"xx\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = WordEmbeddings.pretrained(\"glove_6B_300\", \"xx\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|glove_6B_300|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[xx]|\\n\\n|Dimension:|300|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)\\n'},\n",
       " {'file': '2020-04-22-ner_cellular_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Cellular',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_cellular_en',\n",
       "  'date': '2020-04-22',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for molecular biology related terms. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': 'DNA, Cell_type, Cell_line, RNA, Protein\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_cellular_en_2.4.2_2.4_1587513308751.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_cellular\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_cellular\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_cellular_en_2.4.2_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.2|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on the JNLPBA corpus containing more than 2.404 publication abstracts with 'embeddings_clinical'.\\n\\nhttp://www.geniaproject.org/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.\\n\\n\\n\\n![image](/assets/images/ner_cellular.png)'},\n",
       " {'file': '2020-02-03-wikiner_6B_300_it.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, it, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_IT){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_IT.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_it_2.4.0_2.4_1579717534334.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"it\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"it\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|it|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://it.wikipedia.org](https://it.wikipedia.org)'},\n",
       " {'file': '2019-07-13-wikiner_6B_300_it.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2019-07-13',\n",
       "  'tags': '[open_source, ner, it]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_IT){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_IT.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_it_2.1.0_2.4_1564906405608.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"it\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"it\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.1.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|it|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://it.wikipedia.org](https://it.wikipedia.org)'},\n",
       " {'file': '2020-05-10-wikiner_6B_300_pt.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 300',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_300',\n",
       "  'date': '2020-05-10',\n",
       "  'tags': '[ner, pt, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 300 is trained with GloVe 6B 300 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_PT){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_PT.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_300_pt_2.5.0_2.4_1588495233641.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_300\", \"pt\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_300\", \"pt\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_300|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|pt|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://pt.wikipedia.org](https://pt.wikipedia.org)'},\n",
       " {'file': '2020-04-22-ner_anatomy_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'NerDLModel Anatomy',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'ner_anatomy_en',\n",
       "  'date': '2020-04-22',\n",
       "  'tags': '[ner, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': '\\n\\nPretrained named entity recognition deep learning model for anatomy terms. Includes Anatomical_system, Cell, Cellular_component, Developing_anatomical_structure, Immaterial_anatomical_entity, Multi-tissue_structure, Organ, Organism_subdivision, Organism_substance, Pathological_formation, and Tissue entities. The SparkNLP deep learning model (NerDL) is inspired by a former state of the art model for NER: Chiu & Nicols, Named Entity Recognition with Bidirectional LSTM-CNN. \\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Predicted Entities': 'Anatomical_system,Cell,Cellular_component,Developing_anatomical_structure,Immaterial_anatomical_entity,Multi-tissue_structure,Organ,Organism_subdivision,Organism_substance,Pathological_formation,Tissue\\n\\n\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.Clinical_Named_Entity_Recognition_Model.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_anatomy_en_2.4.2_2.4_1587513307751.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n\\n\\n',\n",
       "  '## How to use': '\\n\\nUse as part of an nlp pipeline with the following stages: DocumentAssembler, SentenceDetector, Tokenizer, WordEmbeddingsModel, NerDLModel. Add the NerConverter to the end of the pipeline to convert entity tokens into full entity chunks.\\n\\n\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n\\n\\n\\n\\n```python\\n\\n\\n\\nclinical_ner = NerDLModel.pretrained(\"ner_anatomy\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nnlpPipeline = Pipeline(stages=[clinical_ner])\\n\\n\\n\\nempty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\\n\\n\\n\\nmodel = nlpPipeline.fit(empty_data)\\n\\n\\n\\nresults = model.transform(data)\\n\\n\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"ner_anatomy\", \"en\", \"clinical/models\") \\\\\\n\\n  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\\\\n\\n  .setOutputCol(\"ner\")\\n\\n\\n\\nval pipeline = new Pipeline().setStages(Array(ner))\\n\\n\\n\\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(data)\\n\\n\\n\\n\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|ner_anatomy_en_2.4.2_2.4|\\n\\n|Type:|ner|\\n\\n|Compatibility:|Spark NLP 2.4.2|\\n\\n|Edition:|Healthcare|\\n\\n|License:|Licensed|\\n\\n|Input Labels:|[sentence,token, embeddings]|\\n\\n|Output Labels:|[ner]|\\n\\n|Language:|[en]|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Dataset used for training': \"Trained on the Anatomical Entity Mention (AnEM) corpus with 'embeddings_clinical'.\\n\\nhttp://www.nactem.ac.uk/anatomy/\\n\\n\\n\\n{:.h2_title}\\n\",\n",
       "  '## Results': 'The output is a dataframe with a sentence per row and a \"ner\" column containing all of the entity labels in the sentence, entity character indices, and other metadata. To get only the tokens and entity labels, without the metadata, select \"token.result\" and \"ner.result\" from your output dataframe or add the \"Finisher\" to the end of your pipeline.me:\\n\\n\\n\\n![image](/assets/images/ner_anatomy.png) '},\n",
       " {'file': '2020-02-03-wikiner_6B_100_es.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'WikiNER 6B 100',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'wikiner_6B_100',\n",
       "  'date': '2020-02-03',\n",
       "  'tags': '[ner, es, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'WikiNER is a Named Entity Recognition (or NER) model, meaning it annotates text to find features like the names of people, places, and organizations. This NER model does not read words directly but instead reads word embeddings, which represent words as points such that more semantically similar words are closer together. WikiNER 6B 100 is trained with GloVe 6B 100 word embeddings, so be sure to use the same embeddings in the pipeline.\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](https://demo.johnsnowlabs.com/public/NER_ES){:.button.button-orange}{:target=\"_blank\"}\\n\\n[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_ES.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/wikiner_6B_100_es_2.4.0_2.4_1581971941700.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nner = NerDLModel.pretrained(\"wikiner_6B_100\", \"es\") \\\\\\n\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\\\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval ner = NerDLModel.pretrained(\"wikiner_6B_100\", \"es\")\\n\\n        .setInputCols(Array(\"document\", \"token\", \"embeddings\"))\\n\\n        .setOutputCol(\"ner\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|wikiner_6B_100|\\n\\n|Type:|ner|\\n\\n|Compatibility:| Spark NLP 2.4.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|sentence, token, embeddings|\\n\\n|Output Labels:|ner|\\n\\n|Language:|es|\\n\\n|Case sensitive:|false|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://es.wikipedia.org](https://es.wikipedia.org)'},\n",
       " {'file': '2020-07-20-biobert_pubmed_large_cased.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'BioBERT Pubmed Large',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'biobert_pubmed_large_cased',\n",
       "  'date': '2020-07-20',\n",
       "  'tags': '[embeddings, en, open_source]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'This model contains a pre-trained weights of BioBERT, a language representation model for biomedical domain, especially designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, question answering, etc. The details are described in the paper \"[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)\".\\n\\n\\n\\n{:.btn-box}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/biobert_pubmed_large_cased_en_2.5.0_2.4_1590487739645.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\n\\n\\nembeddings = BertEmbeddings.pretrained(\"biobert_pubmed_large_cased\", \"en\") \\\\\\n\\n      .setInputCols(\"sentence\", \"token\") \\\\\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n```scala\\n\\n\\n\\nval embeddings = BertEmbeddings.pretrained(\"biobert_pubmed_large_cased\", \"en\")\\n\\n      .setInputCols(\"sentence\", \"token\")\\n\\n      .setOutputCol(\"embeddings\")\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|biobert_pubmed_large_cased|\\n\\n|Type:|embeddings|\\n\\n|Compatibility:|Spark NLP 2.5.0|\\n\\n|License:|Open Source|\\n\\n|Edition:|Official|\\n\\n|Input Labels:|[sentence, token]|\\n\\n|Output Labels:|[word_embeddings]|\\n\\n|Language:|[en]|\\n\\n|Dimension:|1024|\\n\\n|Case sensitive:|true|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Source': 'The model is imported from [https://github.com/dmis-lab/biobert](https://github.com/dmis-lab/biobert)\\n'},\n",
       " {'file': '2020-08-19-explain_clinical_doc_carp_en.md',\n",
       "  'layout': 'model',\n",
       "  'title': 'Explain Clinical Doc CARP',\n",
       "  'author': 'John Snow Labs',\n",
       "  'name': 'explain_clinical_doc_carp_en',\n",
       "  'date': '2020-08-19',\n",
       "  'tags': '[pipeline, en, licensed]',\n",
       "  'article_header': '',\n",
       "  'type': 'cover',\n",
       "  'use_language_switcher': '\"Python-Scala-Java\"',\n",
       "  '## Description': 'A pretrained pipeline with ner_clinical, assertion_dl, re_clinical and ner_posology. It will extract clinical and medication entities, assign assertion status and find relationships between clinical entities.\\n\\n\\n\\n\\n\\n\\n\\n[//]: <[Live Demo](){:.button.button-orange}{:target=\"_blank\"}>\\n\\n\\n\\n{:.btn-box}\\n\\n<button class=\"button button-orange\" disabled>Live Demo</button>\\n\\n[Open in Colab](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/11.Pretrained_Clinical_Pipelines.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}{:target=\"_blank\"}{:target=\"_blank\"}\\n\\n[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/explain_clinical_doc_carp_en_2.5.5_2.4_1597841630062.zip){:.button.button-orange.button-orange-trans.arr.button-icon}\\n\\n\\n',\n",
       "  '## How to use': '\\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\n{% include programmingLanguageSelectScalaPython.html %}\\n\\n\\n\\n```python\\n\\npipeline = PretrainedPipeline(\\'explain_clinical_doc_carp\\', \\'en\\', \\'clinical/models\\')\\n\\n\\n\\nannotations = pipeline.annotate(text)\\n\\n\\n\\nannotations.keys()\\n\\n```\\n\\n\\n\\n{:.noactive}\\n\\n```scala\\n\\n```\\n\\n\\n\\n</div>\\n\\n\\n\\n{:.model-param}\\n',\n",
       "  '## Model Parameters': '\\n\\n{:.table-model}\\n\\n|---|---|\\n\\n|Model Name:|explain_clinical_doc_carp_en_2.5.5_2.4|\\n\\n|Type:|pipeline|\\n\\n|Compatibility:|Spark NLP 2.5.5|\\n\\n|License:|Licensed|\\n\\n|Edition:|Healthcare|\\n\\n|Language:|[en]|\\n\\n\\n\\n{:.h2_title}\\n',\n",
       "  '## Included Models': ' - ner_clinical\\n\\n - assertion_dl\\n\\n - re_clinical\\n\\n - ner_posology\\n\\n{:.h2_title}\\n',\n",
       "  '## Results': \"\\n\\nThe output is a dictionary with the following keys: 'sentences', 'clinical_ner_tags', 'document', 'ner_chunks', 'clinical_ner_chunks', 'ner_tags', 'assertion', 'clinical_relations', 'tokens', 'embeddings', 'pos_tags', 'dependencies'.\\n\"}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels\"] = df[\"## Included Entities\"].mask(pd.isnull,df[\"## Predicted Entities\"].mask(pd.isnull,df[\"## Included Relations\"].mask(pd.isnull,df[\"## Included Assertions\"])))\n",
    "df[\"model_dataset\"] = df[\"## Dataset used for training\"].mask(pd.isnull,df[\"## Source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping = {\n",
      "\"file\":\"xxx\",\n",
      "\"layout\":\"xxx\",\n",
      "\"title\":\"xxx\",\n",
      "\"author\":\"xxx\",\n",
      "\"name\":\"xxx\",\n",
      "\"date\":\"xxx\",\n",
      "\"tags\":\"xxx\",\n",
      "\"article_header\":\"xxx\",\n",
      "\"type\":\"xxx\",\n",
      "\"use_language_switcher\":\"xxx\",\n",
      "\"## Description\":\"xxx\",\n",
      "\"## How to use\":\"xxx\",\n",
      "\"## Model Parameters\":\"xxx\",\n",
      "\"## Source\":\"xxx\",\n",
      "\"## Included Entities\":\"xxx\",\n",
      "\"## Dataset used for training\":\"xxx\",\n",
      "\"## Results\":\"xxx\",\n",
      "\"## Predicted Entities\":\"xxx\",\n",
      "\"## Included Models\":\"xxx\",\n",
      "\"## Included Assertions\":\"xxx\",\n",
      "\"## Included Relations\":\"xxx\",\n",
      "\"labels\":\"xxx\",\n",
      "\"model_dataset\":\"xxx\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"mapping = {\")\n",
    "for c in df.columns:\n",
    "    print('\"'+c+'\":\"xxx\",')\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "\"title\":\"title\",\n",
    "\"labels\":\"labels\",\n",
    "\"author\":\"model_author\",\n",
    "\"name\":\"model_name\",\n",
    "\"date\":\"latest_date\",\n",
    "\"tags\":\"tags\",\n",
    "\"## Description\":\"description\",\n",
    "\"## How to use\":\"code_samples\",\n",
    "\"model_dataset\":\"model_dataset\",\n",
    "\"## Included Models\":\"included_models\",\n",
    "\"## Model Parameters\":\"model_info\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(mapping, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[mapping.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure(x):\n",
    "    ar = [ari.replace(\"|\",\"\").split(\":\") for ari in re.split(\"\\n+\",x) if ari!=\"\" and len(ari.split(\":|\"))>1]\n",
    "    dd = {ari[0]:ari[1] for ari in ar}\n",
    "    return dd\n",
    "    \n",
    "str_info = df.model_info.apply(structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys=[]\n",
    "for s in str_info:\n",
    "    for k in s.keys():\n",
    "        all_keys.append(k)\n",
    "all_keys = set(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in all_keys:\n",
    "    df[k] = str_info.apply(lambda x: x.get(k, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>labels</th>\n",
       "      <th>model_author</th>\n",
       "      <th>model_name</th>\n",
       "      <th>latest_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>description</th>\n",
       "      <th>code_samples</th>\n",
       "      <th>model_dataset</th>\n",
       "      <th>included_models</th>\n",
       "      <th>...</th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Edition</th>\n",
       "      <th>Language</th>\n",
       "      <th>License</th>\n",
       "      <th>Compatibility</th>\n",
       "      <th>Input Labels</th>\n",
       "      <th>Case sensitive</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Output Labels</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, fr, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is trained based on data from [https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>fr</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Deidentification NER (Enriched)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_deid_enriched</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>[ner, en, deidentify, licensed]</td>\n",
       "      <td>Deidentification NER (Enriched) is a Named Ent...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://portal.dbm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>en</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP for Healthcare 2.4.2+</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_deid_enriched</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2019-07-13</td>\n",
       "      <td>[open_source, ner, fr]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is trained based on data from [https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>fr</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.1.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Ner DL Model</td>\n",
       "      <td>Age, Diagnosis, Dosage, Drug_name, Frequency, ...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_jsl_en</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on data gathered and manually annotate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.2</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_jsl_en_2.4.2_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2019-07-13</td>\n",
       "      <td>[open_source, ner, de]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://de.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>de</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.1.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Clinical NER (Large)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_clinical_large</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>Clinical NER (Large) is a Named Entity Recogni...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://portal.dbm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>en</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP for Healthcare 2.5.0+</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_clinical_large</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>[ner, ru, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://ru.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>ru</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.4</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>NerDLModel Bionlp</td>\n",
       "      <td>Amino_acid, Anatomical_system, Cancer, Cell, ...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_bionlp_en</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>[licensed, ner, en]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on Cancer Genetics (CG) task of the Bi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_bionlp_en_2.4.0_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>ALBERT Base Uncase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>albert_base_uncased</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>ALBERT is \"A Lite\" version of BERT, a popular ...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>false</td>\n",
       "      <td>albert_base_uncased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>WikiNER 6B 100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, pt, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://pt.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>pt</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>NerDLModel Posology Small</td>\n",
       "      <td>Dosage, Drug, Duration, Form, Frequency, Route...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_posology_small_en</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on the 2018 i2b2 dataset with 'embeddi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.2</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_posology_small_en_2.4.2_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>GloVe 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>glove_6B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>GloVe (Global Vectors) is a model for distribu...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://nlp.stanfo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>300</td>\n",
       "      <td>Official</td>\n",
       "      <td>en</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence. token</td>\n",
       "      <td>false</td>\n",
       "      <td>glove_6B_300</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Explain Document DL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>explain_document_dl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[pipeline, en, open_source]</td>\n",
       "      <td>The *explain_document_dl* is a pretrained pipe...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The explain_document_ml has one Transformer an...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Community</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>explain_document_dl</td>\n",
       "      <td>None</td>\n",
       "      <td>pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>BERT Large Cased</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>bert_large_cased</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>[open_source, embeddings, en]</td>\n",
       "      <td>This model contains a deep bidirectional trans...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>bert_large_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Universal Sentence Encoder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>tfhub_use</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>The Universal Sentence Encoder encodes text in...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>512</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence]</td>\n",
       "      <td>true</td>\n",
       "      <td>tfhub_use</td>\n",
       "      <td>[sentence_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>BioBERT Clinical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>biobert_clinical_base_cased</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>This model contains a pre-trained weights of C...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://github.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>biobert_clinical_base_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Explain Clinical Doc ERA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>explain_clinical_doc_era_en</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>[pipeline, en, licensed]</td>\n",
       "      <td>A pretrained pipeline with ner_clinical_events...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- ner_clinical_events\\n\\n - assertion_dl\\n\\n ...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.5.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>explain_clinical_doc_era_en_2.5.5_2.4</td>\n",
       "      <td>None</td>\n",
       "      <td>pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, pl, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://pl.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>pl</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>BERT Base Uncased</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>bert_base_uncased</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>[open_source, embeddings, en]</td>\n",
       "      <td>This model contains a deep bidirectional trans...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>false</td>\n",
       "      <td>bert_base_uncased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>XLNet Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>xlnet_large_cased</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>XLNet is a new unsupervised language represent...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://github.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>xlnet_large_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>NerDLModel Clinical Large</td>\n",
       "      <td>Problem, Test, Treatment\\n\\n\\n\\n\\n\\n[//]: &lt;[Li...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_clinical_large_en</td>\n",
       "      <td>2020-05-23</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on augmented 2010 i2b2 challenge data ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licenced</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_clinical_large_en_2.5.0_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, pl, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://pl.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>pl</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, es, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://es.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>es</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>BERT Large Uncased</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>bert_large_uncased</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>[open_source, embeddings, en]</td>\n",
       "      <td>This model contains a deep bidirectional trans...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>false</td>\n",
       "      <td>bert_large_uncased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Deidentification NER (Large)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_deid_large</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>[ner, en, deidentify, licensed]</td>\n",
       "      <td>Deidentification NER (Large) is a Named Entity...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://portal.dbm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>en</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP for Healthcare 2.4.2+</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_deid_large</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>NerDLModel Drugs</td>\n",
       "      <td>- DrugChem\\n\\n\\n\\n[//]: &lt;[Live Demo](){:.butt...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_drugs_en</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on i2b2_med7 + FDA with 'embeddings_cl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.4</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_drugs_en_2.4.4_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, es, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://es.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>es</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Glove 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>glove_840B_300</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>[open_source, embeddings]</td>\n",
       "      <td>GloVe (Global Vectors) is a model for distribu...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://nlp.stanfo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>300</td>\n",
       "      <td>Official</td>\n",
       "      <td>[xx]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>glove_840B_300</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Assertion DL Large</td>\n",
       "      <td>\\n\\n - hypothetical\\n\\n - present\\n\\n - absent...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>assertion_dl_large_en</td>\n",
       "      <td>2020-05-21</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nDeep learning named entity recognition mod...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on 2010 i2b2/VA challenge on concepts,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, ner_chunk, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>assertion_dl_large_en_2.5.0_2.4</td>\n",
       "      <td>[assertion]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>XLNet Base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>xlnet_base_cased</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>XLNet is a new unsupervised language represent...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://github.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>xlnet_base_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>ALBERT XLarge Uncase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>albert_xlarge_uncased</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>ALBERT is \"A Lite\" version of BERT, a popular ...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>false</td>\n",
       "      <td>albert_xlarge_uncased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>ALBERT XXLarge Uncase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>albert_xxlarge_uncased</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>ALBERT is \"A Lite\" version of BERT, a popular ...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>false</td>\n",
       "      <td>albert_xxlarge_uncased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>Wiki NER 6B 100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>2019-07-13</td>\n",
       "      <td>[open_source, ner, de]</td>\n",
       "      <td>Wiki NER is a Named Entity Recognition (or NER...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is trained based on data from [https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>de</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.1.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>BERT Base Cased</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>bert_base_cased</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>[open_source, embeddings, en]</td>\n",
       "      <td>This model contains a deep bidirectional trans...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>bert_base_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, it, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://it.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>it</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>[ner, ru, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://ru.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>ru</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.4</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>ALBERT Large Uncase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>albert_large_uncased</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>ALBERT is \"A Lite\" version of BERT, a popular ...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>false</td>\n",
       "      <td>albert_large_uncased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>Assertion DL</td>\n",
       "      <td>\\n\\n Hypothetical, Present, Absent, Possible, ...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>assertion_dl_en</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>[licensed, ner, en]</td>\n",
       "      <td>\\n\\nDeep learning named entity recognition mod...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on 2010 i2b2/VA challenge on concepts,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, ner_chunk, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>assertion_dl_en_2.4.0_2.4</td>\n",
       "      <td>[assertion]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>NerDLModel Healthcare</td>\n",
       "      <td>Problem, Test, Treatment\\n\\n\\n\\n\\n\\n[//]: &lt;[L...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_healthcare_en</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on 2010 i2b2 challenge data with 'embe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.4</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_healthcare_en_2.4.4_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2019-07-13</td>\n",
       "      <td>[open_source, ner, fr]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is trained based on data from [https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>fr</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.1.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>NerDLModel Risk Factors</td>\n",
       "      <td>CAD, Diabetes, Family_hist, Hyperlipidemia, Hy...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_risk_factors_en</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\n\\n\\nUse as part of an nlp pipeline with th...</td>\n",
       "      <td>Trained on plain n2c2 2014: De-identification ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.2</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_risk_factors_en_2.4.2_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, de, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://de.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>de</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>Deidentify (Large)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>deidentify_large</td>\n",
       "      <td>2020-08-04</td>\n",
       "      <td>[deid, en, licensed]</td>\n",
       "      <td>Deidentify (Large) is a deidentification model...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://portal.dbm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>en</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP for Healthcare 2.5.5</td>\n",
       "      <td>sentence, token, ner_chunk</td>\n",
       "      <td>false</td>\n",
       "      <td>deidentify_large</td>\n",
       "      <td>obfuscated</td>\n",
       "      <td>deid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>Assertion ML</td>\n",
       "      <td>\\n\\n Hypothetical, Present, Absent, Possible, ...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>assertion_ml_en</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>[licensed, ner, en]</td>\n",
       "      <td>\\n\\nLogistic regression based named entity rec...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on 2010 i2b2/VA challenge on concepts,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, ner_chunk, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>assertion_ml_en_2.4.0_2.4</td>\n",
       "      <td>[assertion]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>GloVe 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>glove_840B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>GloVe (Global Vectors) is a model for distribu...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://nlp.stanfo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>300</td>\n",
       "      <td>Official</td>\n",
       "      <td>en</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence. token</td>\n",
       "      <td>false</td>\n",
       "      <td>glove_840B_300</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>WikiNER 6B 100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, nl, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is trained based on data from [https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>nl</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>Ner DL Model Enriched</td>\n",
       "      <td>Age, Diagnosis, Dosage, Drug_name, Frequency, ...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_jsl_enriched_en</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\n\\n\\nUse as part of an nlp pipeline with th...</td>\n",
       "      <td>Trained on data gathered and manually annotate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.2</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_jsl_enriched_en_2.4.2_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2019-07-13</td>\n",
       "      <td>[open_source, ner, it]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://it.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>it</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.1.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>Glove 6B 100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>glove_100d</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>[open_source, embeddings, en]</td>\n",
       "      <td>GloVe (Global Vectors) is a model for distribu...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://nlp.stanfo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>false</td>\n",
       "      <td>glove_100d</td>\n",
       "      <td>[embeddings]</td>\n",
       "      <td>word_embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>NerDLModel Clinical Events</td>\n",
       "      <td>Date,Time,Problem,Test,Treatment,Occurence,Cli...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_events_clinical</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on i2b2 events data with 'clinical_emb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_events_clinical_en_2.5.0_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>Neoplasms NER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_neoplasms</td>\n",
       "      <td>2020-07-03</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>Neoplasms NER is a Named Entity Recognition mo...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://temu.bsc.e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>en</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP for Healthcare 2.5.3+</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_neoplasms</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>BioBERT Discharge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>biobert_discharge_base_cased</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>This model contains a pre-trained weights of C...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://github.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>biobert_discharge_base_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, de, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://de.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>de</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>BioBERT PMC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>biobert_pmc_base_cased</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>This model contains a pre-trained weights of B...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://github.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>biobert_pmc_base_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>WikiNER 6B 100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>[ner, ru, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://ru.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>ru</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.4</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, nl, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is trained based on data from [https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>nl</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, fr, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is trained based on data from [https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>fr</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, nl, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is trained based on data from [https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>nl</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>WikiNER 840B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, pt, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://pt.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>pt</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_840B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>Elmo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>elmo</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>Computes contextualized word representations u...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5121024</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>elmo</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>Onto 100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>onto_100</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, en, open_source]</td>\n",
       "      <td>Onto is a Named Entity Recognition (or NER) mo...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://catalog.ld...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>en</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>onto_100</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>NerDLModel Posology Large</td>\n",
       "      <td>Dosage, Drug, Duration, Form, Frequency, Route...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_posology_large_en</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on the 2018 i2b2 dataset and FDA Drug ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.2</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_posology_large_en_2.4.2_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>NerDLModel Diseases</td>\n",
       "      <td>- Disease\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](#...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_diseases_en</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on i2b2 with 'embeddings_clinical'.\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.4</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_diseases_en_2.4.4_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>RelationExtractionModel Clinical</td>\n",
       "      <td>\\n\\nTrIP: A certain treatment has improved or ...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>re_clinical_en</td>\n",
       "      <td>2020-08-09</td>\n",
       "      <td>[re, en, licensed]</td>\n",
       "      <td>\\n\\nModels the set of clinical relations defin...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on augmented 2010 i2b2 challenge data ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.5.5</td>\n",
       "      <td>[embeddings, pos_tags, ner_chunks, dependencies]</td>\n",
       "      <td>false</td>\n",
       "      <td>re_clinical_en_2.5.5_2.4</td>\n",
       "      <td>[relations]</td>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>Onto 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>onto_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, en, open_source]</td>\n",
       "      <td>Onto is a Named Entity Recognition (or NER) mo...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://catalog.ld...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>en</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>onto_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>WikiNER 6B 100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, pl, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://pl.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>pl</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>NerDLModel Clinical</td>\n",
       "      <td>Problem, Test, Treatment\\n\\n\\n\\n[//]: &lt;[Live ...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_clinical_en</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>[licensed, ner, en]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on 2010 i2b2 challenge data with `embe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_clinical_en_2.4.0_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>BioBERT Pubmed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>biobert_pubmed_base_cased</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>This model contains a pre-trained weights of B...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://github.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>biobert_pubmed_base_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>NerDLModel Posology</td>\n",
       "      <td>Dosage,Drug,Duration,Form,Frequency,Route,Stre...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_posology_en</td>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on the 2018 i2b2 dataset and FDA Drug ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.2</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_posology_en_2.4.2_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>BioBERT Pubmed PMC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>biobert_pubmed_pmc_base_cased</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>This model contains a pre-trained weights of B...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://github.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>biobert_pubmed_pmc_base_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>Universal Sentence Encoder Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>tfhub_use_lg</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>The Universal Sentence Encoder encodes text in...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://tfhub.dev/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>512</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence]</td>\n",
       "      <td>true</td>\n",
       "      <td>tfhub_use_lg</td>\n",
       "      <td>[sentence_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>Glove 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>glove_6B_300</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>[open_source, embeddings]</td>\n",
       "      <td>GloVe (Global Vectors) is a model for distribu...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://nlp.stanfo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>300</td>\n",
       "      <td>Official</td>\n",
       "      <td>[xx]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>false</td>\n",
       "      <td>glove_6B_300</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>NerDLModel Cellular</td>\n",
       "      <td>DNA, Cell_type, Cell_line, RNA, Protein\\n\\n\\n\\...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_cellular_en</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on the JNLPBA corpus containing more t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.2</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_cellular_en_2.4.2_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, it, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://it.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>it</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2019-07-13</td>\n",
       "      <td>[open_source, ner, it]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://it.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>it</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.1.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>WikiNER 6B 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>[ner, pt, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://pt.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>pt</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_300</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>NerDLModel Anatomy</td>\n",
       "      <td>Anatomical_system,Cell,Cellular_component,Deve...</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>ner_anatomy_en</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>[ner, en, licensed]</td>\n",
       "      <td>\\n\\nPretrained named entity recognition deep l...</td>\n",
       "      <td>\\n\\nUse as part of an nlp pipeline with the fo...</td>\n",
       "      <td>Trained on the Anatomical Entity Mention (AnEM...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.4.2</td>\n",
       "      <td>[sentence,token, embeddings]</td>\n",
       "      <td>false</td>\n",
       "      <td>ner_anatomy_en_2.4.2_2.4</td>\n",
       "      <td>[ner]</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>WikiNER 6B 100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[ner, es, open_source]</td>\n",
       "      <td>WikiNER is a Named Entity Recognition (or NER)...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://es.wikiped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Official</td>\n",
       "      <td>es</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.4.0</td>\n",
       "      <td>sentence, token, embeddings</td>\n",
       "      <td>false</td>\n",
       "      <td>wikiner_6B_100</td>\n",
       "      <td>ner</td>\n",
       "      <td>ner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>BioBERT Pubmed Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>biobert_pubmed_large_cased</td>\n",
       "      <td>2020-07-20</td>\n",
       "      <td>[embeddings, en, open_source]</td>\n",
       "      <td>This model contains a pre-trained weights of B...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>The model is imported from [https://github.com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>Official</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Open Source</td>\n",
       "      <td>Spark NLP 2.5.0</td>\n",
       "      <td>[sentence, token]</td>\n",
       "      <td>true</td>\n",
       "      <td>biobert_pubmed_large_cased</td>\n",
       "      <td>[word_embeddings]</td>\n",
       "      <td>embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>Explain Clinical Doc CARP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Snow Labs</td>\n",
       "      <td>explain_clinical_doc_carp_en</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>[pipeline, en, licensed]</td>\n",
       "      <td>A pretrained pipeline with ner_clinical, asser...</td>\n",
       "      <td>\\n\\n&lt;div class=\"tabs-box\" markdown=\"1\"&gt;\\n\\n\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- ner_clinical\\n\\n - assertion_dl\\n\\n - re_cl...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>[en]</td>\n",
       "      <td>Licensed</td>\n",
       "      <td>Spark NLP 2.5.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>explain_clinical_doc_carp_en_2.5.5_2.4</td>\n",
       "      <td>None</td>\n",
       "      <td>pipeline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0                     WikiNER 6B 300   \n",
       "1    Deidentification NER (Enriched)   \n",
       "2                   WikiNER 840B 300   \n",
       "3                       Ner DL Model   \n",
       "4                   WikiNER 840B 300   \n",
       "5               Clinical NER (Large)   \n",
       "6                   WikiNER 840B 300   \n",
       "7                  NerDLModel Bionlp   \n",
       "8                 ALBERT Base Uncase   \n",
       "9                     WikiNER 6B 100   \n",
       "10         NerDLModel Posology Small   \n",
       "11                      GloVe 6B 300   \n",
       "12               Explain Document DL   \n",
       "13                  BERT Large Cased   \n",
       "14        Universal Sentence Encoder   \n",
       "15                  BioBERT Clinical   \n",
       "16          Explain Clinical Doc ERA   \n",
       "17                    WikiNER 6B 300   \n",
       "18                 BERT Base Uncased   \n",
       "19                       XLNet Large   \n",
       "20         NerDLModel Clinical Large   \n",
       "21                  WikiNER 840B 300   \n",
       "22                    WikiNER 6B 300   \n",
       "23                BERT Large Uncased   \n",
       "24      Deidentification NER (Large)   \n",
       "25                  NerDLModel Drugs   \n",
       "26                  WikiNER 840B 300   \n",
       "27                    Glove 840B 300   \n",
       "28                Assertion DL Large   \n",
       "29                        XLNet Base   \n",
       "30              ALBERT XLarge Uncase   \n",
       "31             ALBERT XXLarge Uncase   \n",
       "32                   Wiki NER 6B 100   \n",
       "33                   BERT Base Cased   \n",
       "34                  WikiNER 840B 300   \n",
       "35                    WikiNER 6B 300   \n",
       "36               ALBERT Large Uncase   \n",
       "37                      Assertion DL   \n",
       "38             NerDLModel Healthcare   \n",
       "39                    WikiNER 6B 300   \n",
       "40           NerDLModel Risk Factors   \n",
       "41                  WikiNER 840B 300   \n",
       "42                Deidentify (Large)   \n",
       "43                      Assertion ML   \n",
       "44                    GloVe 840B 300   \n",
       "45                    WikiNER 6B 100   \n",
       "46             Ner DL Model Enriched   \n",
       "47                  WikiNER 840B 300   \n",
       "48                      Glove 6B 100   \n",
       "49        NerDLModel Clinical Events   \n",
       "50                     Neoplasms NER   \n",
       "51                 BioBERT Discharge   \n",
       "52                    WikiNER 6B 300   \n",
       "53                       BioBERT PMC   \n",
       "54                    WikiNER 6B 100   \n",
       "55                    WikiNER 6B 300   \n",
       "56                  WikiNER 840B 300   \n",
       "57                  WikiNER 840B 300   \n",
       "58                  WikiNER 840B 300   \n",
       "59                              Elmo   \n",
       "60                          Onto 100   \n",
       "61         NerDLModel Posology Large   \n",
       "62               NerDLModel Diseases   \n",
       "63  RelationExtractionModel Clinical   \n",
       "64                          Onto 300   \n",
       "65                    WikiNER 6B 100   \n",
       "66               NerDLModel Clinical   \n",
       "67                    BioBERT Pubmed   \n",
       "68               NerDLModel Posology   \n",
       "69                BioBERT Pubmed PMC   \n",
       "70  Universal Sentence Encoder Large   \n",
       "71                      Glove 6B 300   \n",
       "72               NerDLModel Cellular   \n",
       "73                    WikiNER 6B 300   \n",
       "74                    WikiNER 6B 300   \n",
       "75                    WikiNER 6B 300   \n",
       "76                NerDLModel Anatomy   \n",
       "77                    WikiNER 6B 100   \n",
       "78              BioBERT Pubmed Large   \n",
       "79         Explain Clinical Doc CARP   \n",
       "\n",
       "                                               labels    model_author  \\\n",
       "0                                                 NaN  John Snow Labs   \n",
       "1                                                 NaN  John Snow Labs   \n",
       "2                                                 NaN  John Snow Labs   \n",
       "3   Age, Diagnosis, Dosage, Drug_name, Frequency, ...  John Snow Labs   \n",
       "4                                                 NaN  John Snow Labs   \n",
       "5                                                 NaN  John Snow Labs   \n",
       "6                                                 NaN  John Snow Labs   \n",
       "7    Amino_acid, Anatomical_system, Cancer, Cell, ...  John Snow Labs   \n",
       "8                                                 NaN  John Snow Labs   \n",
       "9                                                 NaN  John Snow Labs   \n",
       "10  Dosage, Drug, Duration, Form, Frequency, Route...  John Snow Labs   \n",
       "11                                                NaN  John Snow Labs   \n",
       "12                                                NaN  John Snow Labs   \n",
       "13                                                NaN  John Snow Labs   \n",
       "14                                                NaN  John Snow Labs   \n",
       "15                                                NaN  John Snow Labs   \n",
       "16                                                NaN  John Snow Labs   \n",
       "17                                                NaN  John Snow Labs   \n",
       "18                                                NaN  John Snow Labs   \n",
       "19                                                NaN  John Snow Labs   \n",
       "20  Problem, Test, Treatment\\n\\n\\n\\n\\n\\n[//]: <[Li...  John Snow Labs   \n",
       "21                                                NaN  John Snow Labs   \n",
       "22                                                NaN  John Snow Labs   \n",
       "23                                                NaN  John Snow Labs   \n",
       "24                                                NaN  John Snow Labs   \n",
       "25   - DrugChem\\n\\n\\n\\n[//]: <[Live Demo](){:.butt...  John Snow Labs   \n",
       "26                                                NaN  John Snow Labs   \n",
       "27                                                NaN  John Snow Labs   \n",
       "28  \\n\\n - hypothetical\\n\\n - present\\n\\n - absent...  John Snow Labs   \n",
       "29                                                NaN  John Snow Labs   \n",
       "30                                                NaN  John Snow Labs   \n",
       "31                                                NaN  John Snow Labs   \n",
       "32                                                NaN  John Snow Labs   \n",
       "33                                                NaN  John Snow Labs   \n",
       "34                                                NaN  John Snow Labs   \n",
       "35                                                NaN  John Snow Labs   \n",
       "36                                                NaN  John Snow Labs   \n",
       "37  \\n\\n Hypothetical, Present, Absent, Possible, ...  John Snow Labs   \n",
       "38   Problem, Test, Treatment\\n\\n\\n\\n\\n\\n[//]: <[L...  John Snow Labs   \n",
       "39                                                NaN  John Snow Labs   \n",
       "40  CAD, Diabetes, Family_hist, Hyperlipidemia, Hy...  John Snow Labs   \n",
       "41                                                NaN  John Snow Labs   \n",
       "42                                                NaN  John Snow Labs   \n",
       "43  \\n\\n Hypothetical, Present, Absent, Possible, ...  John Snow Labs   \n",
       "44                                                NaN  John Snow Labs   \n",
       "45                                                NaN  John Snow Labs   \n",
       "46  Age, Diagnosis, Dosage, Drug_name, Frequency, ...  John Snow Labs   \n",
       "47                                                NaN  John Snow Labs   \n",
       "48                                                NaN  John Snow Labs   \n",
       "49  Date,Time,Problem,Test,Treatment,Occurence,Cli...  John Snow Labs   \n",
       "50                                                NaN  John Snow Labs   \n",
       "51                                                NaN  John Snow Labs   \n",
       "52                                                NaN  John Snow Labs   \n",
       "53                                                NaN  John Snow Labs   \n",
       "54                                                NaN  John Snow Labs   \n",
       "55                                                NaN  John Snow Labs   \n",
       "56                                                NaN  John Snow Labs   \n",
       "57                                                NaN  John Snow Labs   \n",
       "58                                                NaN  John Snow Labs   \n",
       "59                                                NaN  John Snow Labs   \n",
       "60                                                NaN  John Snow Labs   \n",
       "61  Dosage, Drug, Duration, Form, Frequency, Route...  John Snow Labs   \n",
       "62   - Disease\\n\\n\\n\\n{:.btn-box}\\n\\n[Live Demo](#...  John Snow Labs   \n",
       "63  \\n\\nTrIP: A certain treatment has improved or ...  John Snow Labs   \n",
       "64                                                NaN  John Snow Labs   \n",
       "65                                                NaN  John Snow Labs   \n",
       "66   Problem, Test, Treatment\\n\\n\\n\\n[//]: <[Live ...  John Snow Labs   \n",
       "67                                                NaN  John Snow Labs   \n",
       "68  Dosage,Drug,Duration,Form,Frequency,Route,Stre...  John Snow Labs   \n",
       "69                                                NaN  John Snow Labs   \n",
       "70                                                NaN  John Snow Labs   \n",
       "71                                                NaN  John Snow Labs   \n",
       "72  DNA, Cell_type, Cell_line, RNA, Protein\\n\\n\\n\\...  John Snow Labs   \n",
       "73                                                NaN  John Snow Labs   \n",
       "74                                                NaN  John Snow Labs   \n",
       "75                                                NaN  John Snow Labs   \n",
       "76  Anatomical_system,Cell,Cellular_component,Deve...  John Snow Labs   \n",
       "77                                                NaN  John Snow Labs   \n",
       "78                                                NaN  John Snow Labs   \n",
       "79                                                NaN  John Snow Labs   \n",
       "\n",
       "                       model_name latest_date  \\\n",
       "0                  wikiner_6B_300  2020-02-03   \n",
       "1               ner_deid_enriched  2020-03-04   \n",
       "2                wikiner_840B_300  2019-07-13   \n",
       "3                      ner_jsl_en  2020-04-22   \n",
       "4                wikiner_840B_300  2019-07-13   \n",
       "5              ner_clinical_large  2020-05-10   \n",
       "6                wikiner_840B_300  2020-03-16   \n",
       "7                   ner_bionlp_en  2020-01-30   \n",
       "8             albert_base_uncased  2020-04-28   \n",
       "9                  wikiner_6B_100  2020-05-10   \n",
       "10          ner_posology_small_en  2020-04-22   \n",
       "11                   glove_6B_300  2020-02-03   \n",
       "12            explain_document_dl         NaN   \n",
       "13               bert_large_cased  2020-01-02   \n",
       "14                      tfhub_use  2020-04-17   \n",
       "15    biobert_clinical_base_cased  2020-07-20   \n",
       "16    explain_clinical_doc_era_en  2020-08-19   \n",
       "17                 wikiner_6B_300  2020-05-10   \n",
       "18              bert_base_uncased  2020-01-02   \n",
       "19              xlnet_large_cased  2020-04-28   \n",
       "20          ner_clinical_large_en  2020-05-23   \n",
       "21               wikiner_840B_300  2020-05-10   \n",
       "22                 wikiner_6B_300  2020-02-03   \n",
       "23             bert_large_uncased  2020-01-02   \n",
       "24                 ner_deid_large  2020-03-04   \n",
       "25                   ner_drugs_en  2020-03-25   \n",
       "26               wikiner_840B_300  2020-02-03   \n",
       "27                 glove_840B_300  2020-01-22   \n",
       "28          assertion_dl_large_en  2020-05-21   \n",
       "29               xlnet_base_cased  2020-04-28   \n",
       "30          albert_xlarge_uncased  2020-04-28   \n",
       "31         albert_xxlarge_uncased  2020-04-28   \n",
       "32                 wikiner_6B_100  2019-07-13   \n",
       "33                bert_base_cased  2020-01-02   \n",
       "34               wikiner_840B_300  2020-02-03   \n",
       "35                 wikiner_6B_300  2020-03-16   \n",
       "36           albert_large_uncased  2020-04-28   \n",
       "37                assertion_dl_en  2020-01-30   \n",
       "38              ner_healthcare_en  2020-03-26   \n",
       "39                 wikiner_6B_300  2019-07-13   \n",
       "40            ner_risk_factors_en  2020-04-22   \n",
       "41               wikiner_840B_300  2020-02-03   \n",
       "42               deidentify_large  2020-08-04   \n",
       "43                assertion_ml_en  2020-01-30   \n",
       "44                 glove_840B_300  2020-02-03   \n",
       "45                 wikiner_6B_100  2020-05-10   \n",
       "46            ner_jsl_enriched_en  2020-04-22   \n",
       "47               wikiner_840B_300  2019-07-13   \n",
       "48                     glove_100d  2020-01-22   \n",
       "49            ner_events_clinical  2020-03-25   \n",
       "50                  ner_neoplasms  2020-07-03   \n",
       "51   biobert_discharge_base_cased  2020-07-20   \n",
       "52                 wikiner_6B_300  2020-02-03   \n",
       "53         biobert_pmc_base_cased  2020-07-20   \n",
       "54                 wikiner_6B_100  2020-03-16   \n",
       "55                 wikiner_6B_300  2020-05-10   \n",
       "56               wikiner_840B_300  2020-02-03   \n",
       "57               wikiner_840B_300  2020-05-10   \n",
       "58               wikiner_840B_300  2020-05-10   \n",
       "59                           elmo  2020-01-31   \n",
       "60                       onto_100  2020-02-03   \n",
       "61          ner_posology_large_en  2020-04-22   \n",
       "62                ner_diseases_en  2020-03-25   \n",
       "63                 re_clinical_en  2020-08-09   \n",
       "64                       onto_300  2020-02-03   \n",
       "65                 wikiner_6B_100  2020-05-10   \n",
       "66                ner_clinical_en  2020-01-30   \n",
       "67      biobert_pubmed_base_cased  2020-07-20   \n",
       "68                ner_posology_en  2020-04-15   \n",
       "69  biobert_pubmed_pmc_base_cased  2020-07-20   \n",
       "70                   tfhub_use_lg  2020-04-17   \n",
       "71                   glove_6B_300  2020-01-22   \n",
       "72                ner_cellular_en  2020-04-22   \n",
       "73                 wikiner_6B_300  2020-02-03   \n",
       "74                 wikiner_6B_300  2019-07-13   \n",
       "75                 wikiner_6B_300  2020-05-10   \n",
       "76                 ner_anatomy_en  2020-04-22   \n",
       "77                 wikiner_6B_100  2020-02-03   \n",
       "78     biobert_pubmed_large_cased  2020-07-20   \n",
       "79   explain_clinical_doc_carp_en  2020-08-19   \n",
       "\n",
       "                               tags  \\\n",
       "0            [ner, fr, open_source]   \n",
       "1   [ner, en, deidentify, licensed]   \n",
       "2            [open_source, ner, fr]   \n",
       "3               [ner, en, licensed]   \n",
       "4            [open_source, ner, de]   \n",
       "5               [ner, en, licensed]   \n",
       "6            [ner, ru, open_source]   \n",
       "7               [licensed, ner, en]   \n",
       "8     [embeddings, en, open_source]   \n",
       "9            [ner, pt, open_source]   \n",
       "10              [ner, en, licensed]   \n",
       "11    [embeddings, en, open_source]   \n",
       "12      [pipeline, en, open_source]   \n",
       "13    [open_source, embeddings, en]   \n",
       "14    [embeddings, en, open_source]   \n",
       "15    [embeddings, en, open_source]   \n",
       "16         [pipeline, en, licensed]   \n",
       "17           [ner, pl, open_source]   \n",
       "18    [open_source, embeddings, en]   \n",
       "19    [embeddings, en, open_source]   \n",
       "20              [ner, en, licensed]   \n",
       "21           [ner, pl, open_source]   \n",
       "22           [ner, es, open_source]   \n",
       "23    [open_source, embeddings, en]   \n",
       "24  [ner, en, deidentify, licensed]   \n",
       "25              [ner, en, licensed]   \n",
       "26           [ner, es, open_source]   \n",
       "27        [open_source, embeddings]   \n",
       "28              [ner, en, licensed]   \n",
       "29    [embeddings, en, open_source]   \n",
       "30    [embeddings, en, open_source]   \n",
       "31    [embeddings, en, open_source]   \n",
       "32           [open_source, ner, de]   \n",
       "33    [open_source, embeddings, en]   \n",
       "34           [ner, it, open_source]   \n",
       "35           [ner, ru, open_source]   \n",
       "36    [embeddings, en, open_source]   \n",
       "37              [licensed, ner, en]   \n",
       "38              [ner, en, licensed]   \n",
       "39           [open_source, ner, fr]   \n",
       "40              [ner, en, licensed]   \n",
       "41           [ner, de, open_source]   \n",
       "42             [deid, en, licensed]   \n",
       "43              [licensed, ner, en]   \n",
       "44    [embeddings, en, open_source]   \n",
       "45           [ner, nl, open_source]   \n",
       "46              [ner, en, licensed]   \n",
       "47           [open_source, ner, it]   \n",
       "48    [open_source, embeddings, en]   \n",
       "49              [ner, en, licensed]   \n",
       "50              [ner, en, licensed]   \n",
       "51    [embeddings, en, open_source]   \n",
       "52           [ner, de, open_source]   \n",
       "53    [embeddings, en, open_source]   \n",
       "54           [ner, ru, open_source]   \n",
       "55           [ner, nl, open_source]   \n",
       "56           [ner, fr, open_source]   \n",
       "57           [ner, nl, open_source]   \n",
       "58           [ner, pt, open_source]   \n",
       "59    [embeddings, en, open_source]   \n",
       "60           [ner, en, open_source]   \n",
       "61              [ner, en, licensed]   \n",
       "62              [ner, en, licensed]   \n",
       "63               [re, en, licensed]   \n",
       "64           [ner, en, open_source]   \n",
       "65           [ner, pl, open_source]   \n",
       "66              [licensed, ner, en]   \n",
       "67    [embeddings, en, open_source]   \n",
       "68              [ner, en, licensed]   \n",
       "69    [embeddings, en, open_source]   \n",
       "70    [embeddings, en, open_source]   \n",
       "71        [open_source, embeddings]   \n",
       "72              [ner, en, licensed]   \n",
       "73           [ner, it, open_source]   \n",
       "74           [open_source, ner, it]   \n",
       "75           [ner, pt, open_source]   \n",
       "76              [ner, en, licensed]   \n",
       "77           [ner, es, open_source]   \n",
       "78    [embeddings, en, open_source]   \n",
       "79         [pipeline, en, licensed]   \n",
       "\n",
       "                                          description  \\\n",
       "0   WikiNER is a Named Entity Recognition (or NER)...   \n",
       "1   Deidentification NER (Enriched) is a Named Ent...   \n",
       "2   WikiNER is a Named Entity Recognition (or NER)...   \n",
       "3   \\n\\nPretrained named entity recognition deep l...   \n",
       "4   WikiNER is a Named Entity Recognition (or NER)...   \n",
       "5   Clinical NER (Large) is a Named Entity Recogni...   \n",
       "6   WikiNER is a Named Entity Recognition (or NER)...   \n",
       "7   \\n\\nPretrained named entity recognition deep l...   \n",
       "8   ALBERT is \"A Lite\" version of BERT, a popular ...   \n",
       "9   WikiNER is a Named Entity Recognition (or NER)...   \n",
       "10  \\n\\nPretrained named entity recognition deep l...   \n",
       "11  GloVe (Global Vectors) is a model for distribu...   \n",
       "12  The *explain_document_dl* is a pretrained pipe...   \n",
       "13  This model contains a deep bidirectional trans...   \n",
       "14  The Universal Sentence Encoder encodes text in...   \n",
       "15  This model contains a pre-trained weights of C...   \n",
       "16  A pretrained pipeline with ner_clinical_events...   \n",
       "17  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "18  This model contains a deep bidirectional trans...   \n",
       "19  XLNet is a new unsupervised language represent...   \n",
       "20  \\n\\nPretrained named entity recognition deep l...   \n",
       "21  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "22  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "23  This model contains a deep bidirectional trans...   \n",
       "24  Deidentification NER (Large) is a Named Entity...   \n",
       "25  \\n\\nPretrained named entity recognition deep l...   \n",
       "26  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "27  GloVe (Global Vectors) is a model for distribu...   \n",
       "28  \\n\\nDeep learning named entity recognition mod...   \n",
       "29  XLNet is a new unsupervised language represent...   \n",
       "30  ALBERT is \"A Lite\" version of BERT, a popular ...   \n",
       "31  ALBERT is \"A Lite\" version of BERT, a popular ...   \n",
       "32  Wiki NER is a Named Entity Recognition (or NER...   \n",
       "33  This model contains a deep bidirectional trans...   \n",
       "34  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "35  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "36  ALBERT is \"A Lite\" version of BERT, a popular ...   \n",
       "37  \\n\\nDeep learning named entity recognition mod...   \n",
       "38  \\n\\nPretrained named entity recognition deep l...   \n",
       "39  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "40  \\n\\nPretrained named entity recognition deep l...   \n",
       "41  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "42  Deidentify (Large) is a deidentification model...   \n",
       "43  \\n\\nLogistic regression based named entity rec...   \n",
       "44  GloVe (Global Vectors) is a model for distribu...   \n",
       "45  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "46  \\n\\nPretrained named entity recognition deep l...   \n",
       "47  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "48  GloVe (Global Vectors) is a model for distribu...   \n",
       "49  \\n\\nPretrained named entity recognition deep l...   \n",
       "50  Neoplasms NER is a Named Entity Recognition mo...   \n",
       "51  This model contains a pre-trained weights of C...   \n",
       "52  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "53  This model contains a pre-trained weights of B...   \n",
       "54  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "55  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "56  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "57  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "58  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "59  Computes contextualized word representations u...   \n",
       "60  Onto is a Named Entity Recognition (or NER) mo...   \n",
       "61  \\n\\nPretrained named entity recognition deep l...   \n",
       "62  \\n\\nPretrained named entity recognition deep l...   \n",
       "63  \\n\\nModels the set of clinical relations defin...   \n",
       "64  Onto is a Named Entity Recognition (or NER) mo...   \n",
       "65  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "66  \\n\\nPretrained named entity recognition deep l...   \n",
       "67  This model contains a pre-trained weights of B...   \n",
       "68  \\n\\nPretrained named entity recognition deep l...   \n",
       "69  This model contains a pre-trained weights of B...   \n",
       "70  The Universal Sentence Encoder encodes text in...   \n",
       "71  GloVe (Global Vectors) is a model for distribu...   \n",
       "72  \\n\\nPretrained named entity recognition deep l...   \n",
       "73  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "74  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "75  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "76  \\n\\nPretrained named entity recognition deep l...   \n",
       "77  WikiNER is a Named Entity Recognition (or NER)...   \n",
       "78  This model contains a pre-trained weights of B...   \n",
       "79  A pretrained pipeline with ner_clinical, asser...   \n",
       "\n",
       "                                         code_samples  \\\n",
       "0   \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "1   \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "2   \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "3   \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "4   \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "5   \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "6   \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "7   \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "8   \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "9   \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "10  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "11  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "12  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "13  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "14  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "15  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "16  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "17  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "18  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "19  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "20  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "21  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "22  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "23  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "24  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "25  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "26  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "27  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "28  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "29  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "30  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "31  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "32  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "33  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "34  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "35  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "36  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "37  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "38  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "39  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "40  \\n\\n\\n\\nUse as part of an nlp pipeline with th...   \n",
       "41  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "42  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "43  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "44  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "45  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "46  \\n\\n\\n\\nUse as part of an nlp pipeline with th...   \n",
       "47  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "48  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "49  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "50  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "51  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "52  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "53  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "54  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "55  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "56  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "57  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "58  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "59  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "60  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "61  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "62  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "63  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "64  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "65  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "66  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "67  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "68  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "69  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "70  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "71  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "72  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "73  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "74  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "75  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "76  \\n\\nUse as part of an nlp pipeline with the fo...   \n",
       "77  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "78  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "79  \\n\\n<div class=\"tabs-box\" markdown=\"1\">\\n\\n\\n\\...   \n",
       "\n",
       "                                        model_dataset  \\\n",
       "0   The model is trained based on data from [https...   \n",
       "1   The model is imported from [https://portal.dbm...   \n",
       "2   The model is trained based on data from [https...   \n",
       "3   Trained on data gathered and manually annotate...   \n",
       "4   The model is imported from [https://de.wikiped...   \n",
       "5   The model is imported from [https://portal.dbm...   \n",
       "6   The model is imported from [https://ru.wikiped...   \n",
       "7   Trained on Cancer Genetics (CG) task of the Bi...   \n",
       "8   The model is imported from [https://tfhub.dev/...   \n",
       "9   The model is imported from [https://pt.wikiped...   \n",
       "10  Trained on the 2018 i2b2 dataset with 'embeddi...   \n",
       "11  The model is imported from [https://nlp.stanfo...   \n",
       "12                                                NaN   \n",
       "13  The model is imported from [https://tfhub.dev/...   \n",
       "14  The model is imported from [https://tfhub.dev/...   \n",
       "15  The model is imported from [https://github.com...   \n",
       "16                                                NaN   \n",
       "17  The model is imported from [https://pl.wikiped...   \n",
       "18  The model is imported from [https://tfhub.dev/...   \n",
       "19  The model is imported from [https://github.com...   \n",
       "20  Trained on augmented 2010 i2b2 challenge data ...   \n",
       "21  The model is imported from [https://pl.wikiped...   \n",
       "22  The model is imported from [https://es.wikiped...   \n",
       "23  The model is imported from [https://tfhub.dev/...   \n",
       "24  The model is imported from [https://portal.dbm...   \n",
       "25  Trained on i2b2_med7 + FDA with 'embeddings_cl...   \n",
       "26  The model is imported from [https://es.wikiped...   \n",
       "27  The model is imported from [https://nlp.stanfo...   \n",
       "28  Trained on 2010 i2b2/VA challenge on concepts,...   \n",
       "29  The model is imported from [https://github.com...   \n",
       "30  The model is imported from [https://tfhub.dev/...   \n",
       "31  The model is imported from [https://tfhub.dev/...   \n",
       "32  The model is trained based on data from [https...   \n",
       "33  The model is imported from [https://tfhub.dev/...   \n",
       "34  The model is imported from [https://it.wikiped...   \n",
       "35  The model is imported from [https://ru.wikiped...   \n",
       "36  The model is imported from [https://tfhub.dev/...   \n",
       "37  Trained on 2010 i2b2/VA challenge on concepts,...   \n",
       "38  Trained on 2010 i2b2 challenge data with 'embe...   \n",
       "39  The model is trained based on data from [https...   \n",
       "40  Trained on plain n2c2 2014: De-identification ...   \n",
       "41  The model is imported from [https://de.wikiped...   \n",
       "42  The model is imported from [https://portal.dbm...   \n",
       "43  Trained on 2010 i2b2/VA challenge on concepts,...   \n",
       "44  The model is imported from [https://nlp.stanfo...   \n",
       "45  The model is trained based on data from [https...   \n",
       "46  Trained on data gathered and manually annotate...   \n",
       "47  The model is imported from [https://it.wikiped...   \n",
       "48  The model is imported from [https://nlp.stanfo...   \n",
       "49  Trained on i2b2 events data with 'clinical_emb...   \n",
       "50  The model is imported from [https://temu.bsc.e...   \n",
       "51  The model is imported from [https://github.com...   \n",
       "52  The model is imported from [https://de.wikiped...   \n",
       "53  The model is imported from [https://github.com...   \n",
       "54  The model is imported from [https://ru.wikiped...   \n",
       "55  The model is trained based on data from [https...   \n",
       "56  The model is trained based on data from [https...   \n",
       "57  The model is trained based on data from [https...   \n",
       "58  The model is imported from [https://pt.wikiped...   \n",
       "59  The model is imported from [https://tfhub.dev/...   \n",
       "60  The model is imported from [https://catalog.ld...   \n",
       "61  Trained on the 2018 i2b2 dataset and FDA Drug ...   \n",
       "62  Trained on i2b2 with 'embeddings_clinical'.\\n\\...   \n",
       "63  Trained on augmented 2010 i2b2 challenge data ...   \n",
       "64  The model is imported from [https://catalog.ld...   \n",
       "65  The model is imported from [https://pl.wikiped...   \n",
       "66  Trained on 2010 i2b2 challenge data with `embe...   \n",
       "67  The model is imported from [https://github.com...   \n",
       "68  Trained on the 2018 i2b2 dataset and FDA Drug ...   \n",
       "69  The model is imported from [https://github.com...   \n",
       "70  The model is imported from [https://tfhub.dev/...   \n",
       "71  The model is imported from [https://nlp.stanfo...   \n",
       "72  Trained on the JNLPBA corpus containing more t...   \n",
       "73  The model is imported from [https://it.wikiped...   \n",
       "74  The model is imported from [https://it.wikiped...   \n",
       "75  The model is imported from [https://pt.wikiped...   \n",
       "76  Trained on the Anatomical Entity Mention (AnEM...   \n",
       "77  The model is imported from [https://es.wikiped...   \n",
       "78  The model is imported from [https://github.com...   \n",
       "79                                                NaN   \n",
       "\n",
       "                                      included_models  ... Dimension  \\\n",
       "0                                                 NaN  ...      None   \n",
       "1                                                 NaN  ...      None   \n",
       "2                                                 NaN  ...      None   \n",
       "3                                                 NaN  ...      None   \n",
       "4                                                 NaN  ...      None   \n",
       "5                                                 NaN  ...      None   \n",
       "6                                                 NaN  ...      None   \n",
       "7                                                 NaN  ...      None   \n",
       "8                                                 NaN  ...       768   \n",
       "9                                                 NaN  ...      None   \n",
       "10                                                NaN  ...      None   \n",
       "11                                                NaN  ...       300   \n",
       "12  The explain_document_ml has one Transformer an...  ...      None   \n",
       "13                                                NaN  ...      1024   \n",
       "14                                                NaN  ...       512   \n",
       "15                                                NaN  ...       768   \n",
       "16   - ner_clinical_events\\n\\n - assertion_dl\\n\\n ...  ...      None   \n",
       "17                                                NaN  ...      None   \n",
       "18                                                NaN  ...       768   \n",
       "19                                                NaN  ...      1024   \n",
       "20                                                NaN  ...      None   \n",
       "21                                                NaN  ...      None   \n",
       "22                                                NaN  ...      None   \n",
       "23                                                NaN  ...      1024   \n",
       "24                                                NaN  ...      None   \n",
       "25                                                NaN  ...      None   \n",
       "26                                                NaN  ...      None   \n",
       "27                                                NaN  ...       300   \n",
       "28                                                NaN  ...      None   \n",
       "29                                                NaN  ...       768   \n",
       "30                                                NaN  ...      2048   \n",
       "31                                                NaN  ...      1024   \n",
       "32                                                NaN  ...      None   \n",
       "33                                                NaN  ...       768   \n",
       "34                                                NaN  ...      None   \n",
       "35                                                NaN  ...      None   \n",
       "36                                                NaN  ...      1024   \n",
       "37                                                NaN  ...      None   \n",
       "38                                                NaN  ...      None   \n",
       "39                                                NaN  ...      None   \n",
       "40                                                NaN  ...      None   \n",
       "41                                                NaN  ...      None   \n",
       "42                                                NaN  ...      None   \n",
       "43                                                NaN  ...      None   \n",
       "44                                                NaN  ...       300   \n",
       "45                                                NaN  ...      None   \n",
       "46                                                NaN  ...      None   \n",
       "47                                                NaN  ...      None   \n",
       "48                                                NaN  ...       100   \n",
       "49                                                NaN  ...      None   \n",
       "50                                                NaN  ...      None   \n",
       "51                                                NaN  ...       768   \n",
       "52                                                NaN  ...      None   \n",
       "53                                                NaN  ...       768   \n",
       "54                                                NaN  ...      None   \n",
       "55                                                NaN  ...      None   \n",
       "56                                                NaN  ...      None   \n",
       "57                                                NaN  ...      None   \n",
       "58                                                NaN  ...      None   \n",
       "59                                                NaN  ...   5121024   \n",
       "60                                                NaN  ...      None   \n",
       "61                                                NaN  ...      None   \n",
       "62                                                NaN  ...      None   \n",
       "63                                                NaN  ...      None   \n",
       "64                                                NaN  ...      None   \n",
       "65                                                NaN  ...      None   \n",
       "66                                                NaN  ...      None   \n",
       "67                                                NaN  ...       768   \n",
       "68                                                NaN  ...      None   \n",
       "69                                                NaN  ...       768   \n",
       "70                                                NaN  ...       512   \n",
       "71                                                NaN  ...       300   \n",
       "72                                                NaN  ...      None   \n",
       "73                                                NaN  ...      None   \n",
       "74                                                NaN  ...      None   \n",
       "75                                                NaN  ...      None   \n",
       "76                                                NaN  ...      None   \n",
       "77                                                NaN  ...      None   \n",
       "78                                                NaN  ...      1024   \n",
       "79   - ner_clinical\\n\\n - assertion_dl\\n\\n - re_cl...  ...      None   \n",
       "\n",
       "       Edition Language      License                      Compatibility  \\\n",
       "0     Official       fr  Open Source                    Spark NLP 2.4.0   \n",
       "1     Official       en     Licensed   Spark NLP for Healthcare 2.4.2+    \n",
       "2     Official       fr  Open Source                    Spark NLP 2.1.0   \n",
       "3   Healthcare     [en]     Licensed                    Spark NLP 2.4.2   \n",
       "4     Official       de  Open Source                    Spark NLP 2.1.0   \n",
       "5     Official       en     Licensed    Spark NLP for Healthcare 2.5.0+   \n",
       "6     Official       ru  Open Source                    Spark NLP 2.4.4   \n",
       "7   Healthcare     [en]     Licensed                    Spark NLP 2.4.0   \n",
       "8     Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "9     Official       pt  Open Source                    Spark NLP 2.5.0   \n",
       "10  Healthcare     [en]     Licensed                    Spark NLP 2.4.2   \n",
       "11    Official       en  Open Source                    Spark NLP 2.4.0   \n",
       "12   Community     [en]  Open Source                    Spark NLP 2.5.5   \n",
       "13    Official     [en]  Open Source                    Spark NLP 2.4.0   \n",
       "14    Official     [en]  Open Source                    Spark NLP 2.4.0   \n",
       "15    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "16  Healthcare     [en]     Licensed                    Spark NLP 2.5.5   \n",
       "17    Official       pl  Open Source                    Spark NLP 2.5.0   \n",
       "18    Official     [en]  Open Source                    Spark NLP 2.4.0   \n",
       "19    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "20  Healthcare     [en]     Licenced                    Spark NLP 2.5.0   \n",
       "21    Official       pl  Open Source                    Spark NLP 2.5.0   \n",
       "22    Official       es  Open Source                    Spark NLP 2.4.0   \n",
       "23    Official     [en]  Open Source                    Spark NLP 2.4.0   \n",
       "24    Official       en     Licensed    Spark NLP for Healthcare 2.4.2+   \n",
       "25  Healthcare     [en]     Licensed                    Spark NLP 2.4.4   \n",
       "26    Official       es  Open Source                    Spark NLP 2.4.0   \n",
       "27    Official     [xx]  Open Source                    Spark NLP 2.4.0   \n",
       "28  Healthcare     [en]     Licensed                    Spark NLP 2.5.0   \n",
       "29    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "30    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "31    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "32    Official       de  Open Source                    Spark NLP 2.1.0   \n",
       "33    Official     [en]  Open Source                    Spark NLP 2.4.0   \n",
       "34    Official       it  Open Source                    Spark NLP 2.4.0   \n",
       "35    Official       ru  Open Source                    Spark NLP 2.4.4   \n",
       "36    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "37  Healthcare     [en]     Licensed                    Spark NLP 2.4.0   \n",
       "38  Healthcare     [en]     Licensed                    Spark NLP 2.4.4   \n",
       "39    Official       fr  Open Source                    Spark NLP 2.1.0   \n",
       "40  Healthcare     [en]     Licensed                    Spark NLP 2.4.2   \n",
       "41    Official       de  Open Source                    Spark NLP 2.4.0   \n",
       "42    Official       en     Licensed     Spark NLP for Healthcare 2.5.5   \n",
       "43  Healthcare     [en]     Licensed                    Spark NLP 2.4.0   \n",
       "44    Official       en  Open Source                    Spark NLP 2.4.0   \n",
       "45    Official       nl  Open Source                    Spark NLP 2.5.0   \n",
       "46  Healthcare     [en]     Licensed                    Spark NLP 2.4.2   \n",
       "47    Official       it  Open Source                    Spark NLP 2.1.0   \n",
       "48    Official     [en]  Open Source                    Spark NLP 2.4.0   \n",
       "49  Healthcare     [en]     Licensed                    Spark NLP 2.5.0   \n",
       "50    Official       en     Licensed    Spark NLP for Healthcare 2.5.3+   \n",
       "51    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "52    Official       de  Open Source                    Spark NLP 2.4.0   \n",
       "53    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "54    Official       ru  Open Source                    Spark NLP 2.4.4   \n",
       "55    Official       nl  Open Source                    Spark NLP 2.5.0   \n",
       "56    Official       fr  Open Source                    Spark NLP 2.4.0   \n",
       "57    Official       nl  Open Source                    Spark NLP 2.5.0   \n",
       "58    Official       pt  Open Source                    Spark NLP 2.5.0   \n",
       "59    Official     [en]  Open Source                    Spark NLP 2.4.0   \n",
       "60    Official       en  Open Source                    Spark NLP 2.4.0   \n",
       "61  Healthcare     [en]     Licensed                    Spark NLP 2.4.2   \n",
       "62  Healthcare     [en]     Licensed                    Spark NLP 2.4.4   \n",
       "63  Healthcare     [en]     Licensed                    Spark NLP 2.5.5   \n",
       "64    Official       en  Open Source                    Spark NLP 2.4.0   \n",
       "65    Official       pl  Open Source                    Spark NLP 2.5.0   \n",
       "66  Healthcare     [en]     Licensed                    Spark NLP 2.4.0   \n",
       "67    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "68  Healthcare     [en]     Licensed                    Spark NLP 2.4.2   \n",
       "69    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "70    Official     [en]  Open Source                    Spark NLP 2.4.0   \n",
       "71    Official     [xx]  Open Source                    Spark NLP 2.4.0   \n",
       "72  Healthcare     [en]     Licensed                    Spark NLP 2.4.2   \n",
       "73    Official       it  Open Source                    Spark NLP 2.4.0   \n",
       "74    Official       it  Open Source                    Spark NLP 2.1.0   \n",
       "75    Official       pt  Open Source                    Spark NLP 2.5.0   \n",
       "76  Healthcare     [en]     Licensed                    Spark NLP 2.4.2   \n",
       "77    Official       es  Open Source                    Spark NLP 2.4.0   \n",
       "78    Official     [en]  Open Source                    Spark NLP 2.5.0   \n",
       "79  Healthcare     [en]     Licensed                    Spark NLP 2.5.5   \n",
       "\n",
       "                                        Input Labels Case sensitive  \\\n",
       "0                        sentence, token, embeddings          false   \n",
       "1                        sentence, token, embeddings          false   \n",
       "2                        sentence, token, embeddings          false   \n",
       "3                       [sentence,token, embeddings]          false   \n",
       "4                        sentence, token, embeddings          false   \n",
       "5                        sentence, token, embeddings          false   \n",
       "6                        sentence, token, embeddings          false   \n",
       "7                       [sentence,token, embeddings]          false   \n",
       "8                                  [sentence, token]          false   \n",
       "9                        sentence, token, embeddings          false   \n",
       "10                      [sentence,token, embeddings]          false   \n",
       "11                                   sentence. token          false   \n",
       "12                                              None           None   \n",
       "13                                 [sentence, token]           true   \n",
       "14                                        [sentence]           true   \n",
       "15                                 [sentence, token]           true   \n",
       "16                                              None           None   \n",
       "17                       sentence, token, embeddings          false   \n",
       "18                                 [sentence, token]          false   \n",
       "19                                 [sentence, token]           true   \n",
       "20                     [sentence, token, embeddings]          false   \n",
       "21                       sentence, token, embeddings          false   \n",
       "22                       sentence, token, embeddings          false   \n",
       "23                                 [sentence, token]          false   \n",
       "24                       sentence, token, embeddings          false   \n",
       "25                      [sentence,token, embeddings]          false   \n",
       "26                       sentence, token, embeddings          false   \n",
       "27                                 [sentence, token]           true   \n",
       "28                 [sentence, ner_chunk, embeddings]          false   \n",
       "29                                 [sentence, token]           true   \n",
       "30                                 [sentence, token]          false   \n",
       "31                                 [sentence, token]          false   \n",
       "32                       sentence, token, embeddings          false   \n",
       "33                                 [sentence, token]           true   \n",
       "34                       sentence, token, embeddings          false   \n",
       "35                       sentence, token, embeddings          false   \n",
       "36                                 [sentence, token]          false   \n",
       "37                 [sentence, ner_chunk, embeddings]          false   \n",
       "38                      [sentence,token, embeddings]          false   \n",
       "39                       sentence, token, embeddings          false   \n",
       "40                      [sentence,token, embeddings]          false   \n",
       "41                       sentence, token, embeddings          false   \n",
       "42                        sentence, token, ner_chunk          false   \n",
       "43                 [sentence, ner_chunk, embeddings]          false   \n",
       "44                                   sentence. token          false   \n",
       "45                       sentence, token, embeddings          false   \n",
       "46                      [sentence,token, embeddings]          false   \n",
       "47                       sentence, token, embeddings          false   \n",
       "48                                 [sentence, token]          false   \n",
       "49                      [sentence,token, embeddings]          false   \n",
       "50                       sentence, token, embeddings          false   \n",
       "51                                 [sentence, token]           true   \n",
       "52                       sentence, token, embeddings          false   \n",
       "53                                 [sentence, token]           true   \n",
       "54                       sentence, token, embeddings          false   \n",
       "55                       sentence, token, embeddings          false   \n",
       "56                       sentence, token, embeddings          false   \n",
       "57                       sentence, token, embeddings          false   \n",
       "58                       sentence, token, embeddings          false   \n",
       "59                                 [sentence, token]           true   \n",
       "60                       sentence, token, embeddings          false   \n",
       "61                      [sentence,token, embeddings]          false   \n",
       "62                      [sentence,token, embeddings]          false   \n",
       "63  [embeddings, pos_tags, ner_chunks, dependencies]          false   \n",
       "64                       sentence, token, embeddings          false   \n",
       "65                       sentence, token, embeddings          false   \n",
       "66                      [sentence,token, embeddings]          false   \n",
       "67                                 [sentence, token]           true   \n",
       "68                      [sentence,token, embeddings]          false   \n",
       "69                                 [sentence, token]           true   \n",
       "70                                        [sentence]           true   \n",
       "71                                 [sentence, token]          false   \n",
       "72                      [sentence,token, embeddings]          false   \n",
       "73                       sentence, token, embeddings          false   \n",
       "74                       sentence, token, embeddings          false   \n",
       "75                       sentence, token, embeddings          false   \n",
       "76                      [sentence,token, embeddings]          false   \n",
       "77                       sentence, token, embeddings          false   \n",
       "78                                 [sentence, token]           true   \n",
       "79                                              None           None   \n",
       "\n",
       "                                Model Name          Output Labels  \\\n",
       "0                           wikiner_6B_300                    ner   \n",
       "1                        ner_deid_enriched                    ner   \n",
       "2                         wikiner_840B_300                    ner   \n",
       "3                     ner_jsl_en_2.4.2_2.4                  [ner]   \n",
       "4                         wikiner_840B_300                    ner   \n",
       "5                       ner_clinical_large                    ner   \n",
       "6                         wikiner_840B_300                    ner   \n",
       "7                  ner_bionlp_en_2.4.0_2.4                  [ner]   \n",
       "8                      albert_base_uncased      [word_embeddings]   \n",
       "9                           wikiner_6B_100                    ner   \n",
       "10         ner_posology_small_en_2.4.2_2.4                  [ner]   \n",
       "11                            glove_6B_300             embeddings   \n",
       "12                     explain_document_dl                   None   \n",
       "13                        bert_large_cased      [word_embeddings]   \n",
       "14                               tfhub_use  [sentence_embeddings]   \n",
       "15             biobert_clinical_base_cased      [word_embeddings]   \n",
       "16   explain_clinical_doc_era_en_2.5.5_2.4                   None   \n",
       "17                          wikiner_6B_300                    ner   \n",
       "18                       bert_base_uncased      [word_embeddings]   \n",
       "19                       xlnet_large_cased      [word_embeddings]   \n",
       "20         ner_clinical_large_en_2.5.0_2.4                  [ner]   \n",
       "21                        wikiner_840B_300                    ner   \n",
       "22                          wikiner_6B_300                    ner   \n",
       "23                      bert_large_uncased      [word_embeddings]   \n",
       "24                          ner_deid_large                    ner   \n",
       "25                  ner_drugs_en_2.4.4_2.4                  [ner]   \n",
       "26                        wikiner_840B_300                    ner   \n",
       "27                          glove_840B_300      [word_embeddings]   \n",
       "28         assertion_dl_large_en_2.5.0_2.4            [assertion]   \n",
       "29                        xlnet_base_cased      [word_embeddings]   \n",
       "30                   albert_xlarge_uncased      [word_embeddings]   \n",
       "31                  albert_xxlarge_uncased      [word_embeddings]   \n",
       "32                          wikiner_6B_100                    ner   \n",
       "33                         bert_base_cased      [word_embeddings]   \n",
       "34                        wikiner_840B_300                    ner   \n",
       "35                          wikiner_6B_300                    ner   \n",
       "36                    albert_large_uncased      [word_embeddings]   \n",
       "37               assertion_dl_en_2.4.0_2.4            [assertion]   \n",
       "38             ner_healthcare_en_2.4.4_2.4                  [ner]   \n",
       "39                          wikiner_6B_300                    ner   \n",
       "40           ner_risk_factors_en_2.4.2_2.4                  [ner]   \n",
       "41                        wikiner_840B_300                    ner   \n",
       "42                        deidentify_large             obfuscated   \n",
       "43               assertion_ml_en_2.4.0_2.4            [assertion]   \n",
       "44                          glove_840B_300             embeddings   \n",
       "45                          wikiner_6B_100                    ner   \n",
       "46           ner_jsl_enriched_en_2.4.2_2.4                  [ner]   \n",
       "47                        wikiner_840B_300                    ner   \n",
       "48                              glove_100d           [embeddings]   \n",
       "49        ner_events_clinical_en_2.5.0_2.4                  [ner]   \n",
       "50                           ner_neoplasms                    ner   \n",
       "51            biobert_discharge_base_cased      [word_embeddings]   \n",
       "52                          wikiner_6B_300                    ner   \n",
       "53                  biobert_pmc_base_cased      [word_embeddings]   \n",
       "54                          wikiner_6B_100                    ner   \n",
       "55                          wikiner_6B_300                    ner   \n",
       "56                        wikiner_840B_300                    ner   \n",
       "57                        wikiner_840B_300                    ner   \n",
       "58                        wikiner_840B_300                    ner   \n",
       "59                                    elmo      [word_embeddings]   \n",
       "60                                onto_100                    ner   \n",
       "61         ner_posology_large_en_2.4.2_2.4                  [ner]   \n",
       "62               ner_diseases_en_2.4.4_2.4                  [ner]   \n",
       "63                re_clinical_en_2.5.5_2.4            [relations]   \n",
       "64                                onto_300                    ner   \n",
       "65                          wikiner_6B_100                    ner   \n",
       "66               ner_clinical_en_2.4.0_2.4                  [ner]   \n",
       "67               biobert_pubmed_base_cased      [word_embeddings]   \n",
       "68               ner_posology_en_2.4.2_2.4                  [ner]   \n",
       "69           biobert_pubmed_pmc_base_cased      [word_embeddings]   \n",
       "70                            tfhub_use_lg  [sentence_embeddings]   \n",
       "71                            glove_6B_300      [word_embeddings]   \n",
       "72               ner_cellular_en_2.4.2_2.4                  [ner]   \n",
       "73                          wikiner_6B_300                    ner   \n",
       "74                          wikiner_6B_300                    ner   \n",
       "75                          wikiner_6B_300                    ner   \n",
       "76                ner_anatomy_en_2.4.2_2.4                  [ner]   \n",
       "77                          wikiner_6B_100                    ner   \n",
       "78              biobert_pubmed_large_cased      [word_embeddings]   \n",
       "79  explain_clinical_doc_carp_en_2.5.5_2.4                   None   \n",
       "\n",
       "               Type  \n",
       "0               ner  \n",
       "1               ner  \n",
       "2               ner  \n",
       "3               ner  \n",
       "4               ner  \n",
       "5               ner  \n",
       "6               ner  \n",
       "7               ner  \n",
       "8        embeddings  \n",
       "9               ner  \n",
       "10              ner  \n",
       "11       embeddings  \n",
       "12         pipeline  \n",
       "13       embeddings  \n",
       "14       embeddings  \n",
       "15       embeddings  \n",
       "16         pipeline  \n",
       "17              ner  \n",
       "18       embeddings  \n",
       "19       embeddings  \n",
       "20              ner  \n",
       "21              ner  \n",
       "22              ner  \n",
       "23       embeddings  \n",
       "24              ner  \n",
       "25              ner  \n",
       "26              ner  \n",
       "27       embeddings  \n",
       "28              ner  \n",
       "29       embeddings  \n",
       "30       embeddings  \n",
       "31       embeddings  \n",
       "32              ner  \n",
       "33       embeddings  \n",
       "34              ner  \n",
       "35              ner  \n",
       "36       embeddings  \n",
       "37              ner  \n",
       "38              ner  \n",
       "39              ner  \n",
       "40              ner  \n",
       "41              ner  \n",
       "42             deid  \n",
       "43              ner  \n",
       "44       embeddings  \n",
       "45              ner  \n",
       "46              ner  \n",
       "47              ner  \n",
       "48  word_embeddings  \n",
       "49              ner  \n",
       "50              ner  \n",
       "51       embeddings  \n",
       "52              ner  \n",
       "53       embeddings  \n",
       "54              ner  \n",
       "55              ner  \n",
       "56              ner  \n",
       "57              ner  \n",
       "58              ner  \n",
       "59       embeddings  \n",
       "60              ner  \n",
       "61              ner  \n",
       "62              ner  \n",
       "63               re  \n",
       "64              ner  \n",
       "65              ner  \n",
       "66              ner  \n",
       "67       embeddings  \n",
       "68              ner  \n",
       "69       embeddings  \n",
       "70       embeddings  \n",
       "71       embeddings  \n",
       "72              ner  \n",
       "73              ner  \n",
       "74              ner  \n",
       "75              ner  \n",
       "76              ner  \n",
       "77              ner  \n",
       "78       embeddings  \n",
       "79         pipeline  \n",
       "\n",
       "[80 rows x 21 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"docs_module/metadata/model_metadata_existing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsl368",
   "language": "python",
   "name": "jsl368"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
